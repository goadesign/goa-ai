package runtime

import (
	"context"
	"errors"
	"fmt"
	"strings"
	"time"

	"github.com/google/uuid"
	"goa.design/goa-ai/runtime/agent"
	"goa.design/goa-ai/runtime/agent/engine"
	"goa.design/goa-ai/runtime/agent/hooks"
	"goa.design/goa-ai/runtime/agent/interrupt"
	"goa.design/goa-ai/runtime/agent/memory"
	"goa.design/goa-ai/runtime/agent/model"
	"goa.design/goa-ai/runtime/agent/planner"
	"goa.design/goa-ai/runtime/agent/policy"
	"goa.design/goa-ai/runtime/agent/run"
	"goa.design/goa-ai/runtime/agent/telemetry"
	"goa.design/goa-ai/runtime/agent/tools"
	"goa.design/goa-ai/runtime/agent/transcript"
)

const (
	// Minimal viable timeout for scheduling an activity. If remaining time is
	// less than or equal to this value, the runtime should not schedule new work.
	minActivityTimeout = 3 * time.Second

	// Minimal viable grace period for finalization. If remaining time is
	// less than or equal to this value, the runtime should finalize with a user-facing message.
	defaultFinalizerGrace = 10 * time.Second
)

type (
	// futureInfo bundles a Future with its associated tool call metadata for parallel execution.
	// When tools are launched asynchronously via ExecuteActivityAsync, we need to track the
	// future handle alongside the original call details and start time so we can correlate
	// results and measure duration when collecting completed activities.
	futureInfo struct {
		// future is the engine Future returned by ExecuteActivityAsync for this tool call.
		future engine.Future
		// call is the original tool request that was submitted for execution.
		call planner.ToolRequest
		// startTime records when the activity was scheduled, used to calculate tool duration.
		startTime time.Time
	}

	// turnSequencer tracks the turn ID and monotonic sequence counter for event ordering.
	// Each run maintains its own sequencer to ensure deterministic event ordering within
	// a conversational turn.
	turnSequencer struct {
		turnID   string
		sequence int
	}

	// childTracker tracks dynamically discovered child tool calls for a parent tool
	// (agent-as-tool pattern). As the planner discovers new child tools across iterations,
	// the tracker maintains the unique set of discovered IDs and triggers update events
	// when the count increases. This enables UI progress tracking ("3 of 5 complete").
	//
	// NOTE: This infrastructure is currently unused and reserved for future implementation
	// of nested agent-as-tool workflows where tools run their own internal planning loops
	// and dynamically discover child tools across multiple iterations. The struct and
	// methods are defined now to support future codegen and maintain API stability.
	childTracker struct {
		// parentToolCallID identifies the parent tool (usually an agent-as-tool invocation).
		parentToolCallID string
		// discovered maps tool call IDs to struct{} for efficient membership checking.
		// The map size is the current expected children total.
		discovered map[string]struct{}
		// lastExpectedTotal is the count last reported via ToolCallUpdatedEvent.
		// We only emit update events when len(discovered) > lastExpectedTotal.
		lastExpectedTotal int
	}
)

// ExecuteWorkflow is the main entry point for the generated workflow handler.
//
// Advanced & generated integration
//   - Intended to be invoked by code generated by goa-ai during agent
//     registration, or by advanced users writing custom engine adapters.
//   - Normal applications should prefer the high‑level AgentClient API
//     (Runtime.Client(...).Run/Start) rather than calling this directly.
//
// It executes the agent's plan/tool loop using the configured planner, policy,
// and runtime hooks. Returns the final agent output or an error if the workflow
// fails. Generated code calls this from the workflow handler registered with
// the engine.
func (r *Runtime) ExecuteWorkflow(wfCtx engine.WorkflowContext, input *RunInput) (*RunOutput, error) {
	if r.logger != nil {
		r.logger.Info(wfCtx.Context(), "ExecuteWorkflow called", "agent_id", input.AgentID, "run_id", input.RunID)
	}
	if input.AgentID == "" {
		return nil, errors.New("agent id is required")
	}
	defer func() {
		r.storeWorkflowHandle(input.RunID, nil)
		if r.reminders != nil {
			r.reminders.ClearRun(input.RunID)
		}
	}()
	reg, ok := r.agentByID(input.AgentID)
	if !ok {
		return nil, fmt.Errorf("agent %q is not registered", input.AgentID)
	}
	r.logger.Info(wfCtx.Context(), "Agent found, executing plan activity", "agent_id", input.AgentID)
	ctrl := interrupt.NewController(wfCtx)
	reader := r.memoryReader(wfCtx.Context(), string(input.AgentID), input.RunID)
	agentCtx := newAgentContext(agentContextOptions{
		runtime: r,
		agentID: input.AgentID,
		runID:   input.RunID,
		memory:  reader,
		turnID:  input.TurnID,
	})
	runCtx := run.Context{
		RunID:            input.RunID,
		SessionID:        input.SessionID,
		TurnID:           input.TurnID,
		ParentToolCallID: input.ParentToolCallID,
		ParentRunID:      input.ParentRunID,
		ParentAgentID:    input.ParentAgentID,
		Tool:             input.Tool,
		ToolArgs:         input.ToolArgs,
		Attempt:          1,
		Labels:           input.Labels,
	}
	// Initialize turn sequencer for event ordering if TurnID is provided
	var seq *turnSequencer
	if input.TurnID != "" {
		seq = &turnSequencer{
			turnID: input.TurnID,
		}
	}
	r.publishHook(
		wfCtx.Context(),
		hooks.NewRunStartedEvent(input.RunID, input.AgentID, runCtx, *input),
		seq,
	)
	// Initial phase: input has been received and planning is about to begin.
	r.publishHook(
		wfCtx.Context(),
		hooks.NewRunPhaseChangedEvent(input.RunID, input.AgentID, input.SessionID, run.PhasePrompted),
		seq,
	)
	r.recordRunStatus(wfCtx.Context(), input, run.StatusRunning, nil)
	// Track final run outcome for RunCompletedEvent so streaming and observability
	// see accurate success/failed/canceled status instead of always "success".
	const (
		runStatusSuccess  = "success"
		runStatusFailed   = "failed"
		runStatusCanceled = "canceled"
	)
	finalStatus := runStatusSuccess
	var finalErr error
	defer func() {
		// Emit a terminal phase change before RunCompleted so streams see the
		// final phase alongside the completion event.
		var phase run.Phase
		switch finalStatus {
		case runStatusSuccess:
			phase = run.PhaseCompleted
		case runStatusFailed:
			phase = run.PhaseFailed
		case runStatusCanceled:
			phase = run.PhaseCanceled
		default:
			phase = run.PhaseCompleted
		}
		r.publishHook(
			wfCtx.Context(),
			hooks.NewRunPhaseChangedEvent(input.RunID, input.AgentID, input.SessionID, phase),
			seq,
		)
		r.publishHook(
			wfCtx.Context(),
			hooks.NewRunCompletedEvent(input.RunID, input.AgentID, input.SessionID, finalStatus, phase, finalErr),
			seq,
		)
	}()

	planInput := &planner.PlanInput{
		Messages:   input.Messages,
		RunContext: runCtx,
		Agent:      agentCtx,
		Events:     newPlannerEvents(r, input.AgentID, input.RunID, input.SessionID),
	}
	// Compute deadlines before the initial Plan so it cannot outlive the run window.
	var (
		timeBudget     time.Duration
		budgetDeadline time.Time
		hardDeadline   time.Time
		grace          time.Duration
	)
	{
		timeBudget = time.Duration(0)
		if input.Policy != nil && input.Policy.TimeBudget > 0 {
			timeBudget = input.Policy.TimeBudget
		} else if reg.Policy.TimeBudget > 0 {
			timeBudget = reg.Policy.TimeBudget
		}
		switch {
		case input.Policy != nil && input.Policy.FinalizerGrace > 0:
			grace = input.Policy.FinalizerGrace
		case reg.Policy.FinalizerGrace > 0:
			grace = reg.Policy.FinalizerGrace
		default:
			grace = defaultFinalizerGrace
		}
		if timeBudget > 0 {
			budgetDeadline = wfCtx.Now().Add(timeBudget)
			hardDeadline = budgetDeadline.Add(grace)
		}
	}
	startReq := PlanActivityInput{
		AgentID:    input.AgentID,
		RunID:      input.RunID,
		Messages:   input.Messages,
		RunContext: runCtx,
	}
	// Apply run-level Plan timeout override when provided.
	planOpts := reg.PlanActivityOptions
	if input.Policy != nil && input.Policy.PlanTimeout > 0 {
		planOpts.Timeout = input.Policy.PlanTimeout
	}
	// Emit timing resolution for observability.
	if r.logger != nil {
		var planTimeout time.Duration
		if planOpts.Timeout > 0 {
			planTimeout = planOpts.Timeout
		}
		toolTimeout := reg.ExecuteToolActivityOptions.Timeout
		if toolTimeout == 0 {
			toolTimeout = 2 * time.Minute
		}
		if input.Policy != nil && input.Policy.ToolTimeout > 0 {
			toolTimeout = input.Policy.ToolTimeout
		}
		r.logger.Info(wfCtx.Context(), "timing_resolved",
			"time_budget", timeBudget,
			"finalizer_grace", grace,
			"hard_deadline", hardDeadline,
			"plan_timeout", planTimeout,
			"tool_timeout", toolTimeout,
		)
	}
	// Transition into planning before invoking the planner activity.
	r.publishHook(
		wfCtx.Context(),
		hooks.NewRunPhaseChangedEvent(input.RunID, input.AgentID, input.SessionID, run.PhasePlanning),
		seq,
	)
	firstOutput, err := r.runPlanActivity(wfCtx, reg.PlanActivityName, planOpts, startReq, hardDeadline)
	if err != nil {
		r.logger.Error(wfCtx.Context(), "Plan activity failed", "error", err)
		finalErr = err
		if errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded) {
			finalStatus = runStatusCanceled
		} else {
			finalStatus = runStatusFailed
		}
		r.recordRunStatus(wfCtx.Context(), input, run.StatusFailed, map[string]any{
			"error": err.Error(),
		})
		return nil, err
	}
	if firstOutput == nil || firstOutput.Result == nil {
		r.logger.Error(wfCtx.Context(), "Plan activity returned nil result")
		finalErr = fmt.Errorf("CRITICAL: Plan activity returned nil PlanResult")
		finalStatus = runStatusFailed
		r.recordRunStatus(wfCtx.Context(), input, run.StatusFailed, map[string]any{
			"error": "CRITICAL: Plan activity returned nil PlanResult",
		})
		return nil, finalErr
	}
	result := firstOutput.Result
	r.logger.Info(wfCtx.Context(), "Plan activity completed", "tool_calls", len(result.ToolCalls), "final_response", result.FinalResponse != nil)
	// CRITICAL: Validate PlanResult structure - if planner returned ToolCalls, they should be present
	if len(result.ToolCalls) == 0 && result.FinalResponse == nil && result.Await == nil {
		finalErr = fmt.Errorf("CRITICAL: PlanResult has no ToolCalls, FinalResponse, or Await - this should never happen")
		finalStatus = runStatusFailed
		r.recordRunStatus(wfCtx.Context(), input, run.StatusFailed, map[string]any{
			"error": "CRITICAL: PlanResult has no ToolCalls, FinalResponse, or Await",
		})
		return nil, finalErr
	}
	// CRITICAL: If ToolCalls is empty but planner returned them, serialization may have failed
	if len(result.ToolCalls) == 0 && result.FinalResponse != nil {
		r.logger.Info(wfCtx.Context(), "PlanResult has FinalResponse but no ToolCalls - workflow will return early")
	}
	caps := initialCaps(reg.Policy)
	// Apply per-run cap overrides (run-level)
	if input.Policy != nil {
		if input.Policy.MaxToolCalls > 0 {
			caps.MaxToolCalls = input.Policy.MaxToolCalls
			caps.RemainingToolCalls = input.Policy.MaxToolCalls
		}
		if input.Policy.MaxConsecutiveFailedToolCalls > 0 {
			caps.MaxConsecutiveFailedToolCalls = input.Policy.MaxConsecutiveFailedToolCalls
			caps.RemainingConsecutiveFailedToolCalls = input.Policy.MaxConsecutiveFailedToolCalls
		}
	}
	// Deadlines (budgetDeadline, hardDeadline, grace) already computed above.
	nextAttempt := planInput.RunContext.Attempt + 1
	r.logger.Info(wfCtx.Context(), "Starting runLoop", "tool_calls", len(result.ToolCalls))
	// Create parentTracker if this is a nested agent run (has ParentToolCallID)
	var parentTracker *childTracker
	if planInput.RunContext.ParentToolCallID != "" {
		parentTracker = newChildTracker(planInput.RunContext.ParentToolCallID)
	}
	// Enter tool execution phase for the main run loop.
	r.publishHook(
		wfCtx.Context(),
		hooks.NewRunPhaseChangedEvent(input.RunID, input.AgentID, input.SessionID, run.PhaseExecutingTools),
		seq,
	)
	out, err := r.runLoop(
		wfCtx,
		reg,
		input,
		planInput,
		firstOutput.Result,
		firstOutput.Transcript,
		caps,
		hardDeadline,
		nextAttempt,
		seq,
		parentTracker,
		ctrl,
		grace,
	)
	if err != nil {
		finalErr = err
		if errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded) {
			finalStatus = runStatusCanceled
		} else {
			finalStatus = runStatusFailed
		}
		r.recordRunStatus(wfCtx.Context(), input, run.StatusFailed, map[string]any{
			"error": err.Error(),
		})
		return nil, err
	}
	// Successful completion.
	finalStatus = runStatusSuccess
	finalErr = nil
	r.recordRunStatus(wfCtx.Context(), input, run.StatusCompleted, nil)
	return out, nil
}

// runLoop executes the plan/tool/resume cycle until the planner returns a final response
// or a cap/deadline is exceeded. The seq parameter enables turn-based event sequencing.
func (r *Runtime) runLoop(
	wfCtx engine.WorkflowContext,
	reg AgentRegistration,
	input *RunInput,
	base *planner.PlanInput,
	initialResult *planner.PlanResult,
	initialTranscript []*model.Message,
	caps policy.CapsState,
	deadline time.Time,
	nextAttempt int,
	seq *turnSequencer,
	parentTracker *childTracker,
	ctrl *interrupt.Controller,
	finalizerGrace time.Duration,
) (*RunOutput, error) {
	if base == nil {
		return nil, errors.New("base plan input is required")
	}
	ctx := wfCtx.Context()
	if r.logger == nil {
		r.logger = telemetry.NoopLogger{}
	}
	// Aggregate usage during the run via a temporary subscriber.
	var aggUsage model.TokenUsage
	usageSub := r.registerUsageAggregator(ctx, input.RunID, string(input.AgentID), &aggUsage)
	if usageSub != nil {
		defer func() {
			_ = usageSub.Close()
		}()
	}
	if initialResult == nil {
		return nil, fmt.Errorf("CRITICAL: runLoop initial PlanResult is nil")
	}
	if len(initialResult.ToolCalls) == 0 && initialResult.FinalResponse == nil && initialResult.Await == nil {
		return nil, fmt.Errorf("CRITICAL: runLoop initial PlanResult has no ToolCalls, FinalResponse, or Await")
	}
	r.logger.Info(ctx, "runLoop starting iteration", "tool_calls", len(initialResult.ToolCalls), "final_response", initialResult.FinalResponse != nil, "await", initialResult.Await != nil)
	result := initialResult
	turnTranscript := initialTranscript
	// Initialize provider transcript ledger for this run/turn.
	led := transcript.FromModelMessages(turnTranscript)
	// Expose provider-ready messages via a workflow query for external rehydration.
	_ = wfCtx.SetQueryHandler("ledger_messages", func() ([]*model.Message, error) {
		return led.BuildMessages(), nil
	})
	// Derive per-run overrides for Resume and Tools.
	resumeOpts := reg.ResumeActivityOptions
	if input != nil && input.Policy != nil && input.Policy.PlanTimeout > 0 {
		resumeOpts.Timeout = input.Policy.PlanTimeout
	}
	toolOpts := reg.ExecuteToolActivityOptions
	if input != nil && input.Policy != nil && input.Policy.ToolTimeout > 0 {
		toolOpts.Timeout = input.Policy.ToolTimeout
	}
	if toolOpts.Timeout == 0 {
		toolOpts.Timeout = 2 * time.Minute
	}
	var lastToolResults []*planner.ToolResult
	var aggregatedToolResults []*planner.ToolResult
	for {
		if err := r.handleInterrupts(wfCtx, input, base, seq, ctrl, &nextAttempt); err != nil {
			return nil, err
		}
		// When a hard deadline is set, stop scheduling new work when the remaining time
		// is less than or equal to the grace period; request finalization instead.
		if !deadline.IsZero() {
			now := wfCtx.Now()
			remaining := deadline.Sub(now)
			if finalizerGrace > 0 && remaining <= finalizerGrace {
				return r.finalizeWithPlanner(wfCtx, reg, input, base, aggregatedToolResults, nextAttempt, seq, planner.TerminationReasonTimeBudget, deadline)
			}
			if finalizerGrace == 0 && remaining <= minActivityTimeout {
				// No configured grace; avoid scheduling work that cannot complete meaningfully.
				return r.finalizeWithPlanner(wfCtx, reg, input, base, aggregatedToolResults, nextAttempt, seq, planner.TerminationReasonTimeBudget, deadline)
			}
			if remaining <= 0 {
				// Time budget fully exhausted (including grace): try to finalize.
				// This may still race with engine TTLs, but provides a best-effort reply.
				return r.finalizeWithPlanner(wfCtx, reg, input, base, aggregatedToolResults, nextAttempt, seq, planner.TerminationReasonTimeBudget, deadline)
			}
		}
		if !deadline.IsZero() && wfCtx.Now().After(deadline) {
			// Redundant guard; kept for clarity with prior semantics.
			return r.finalizeWithPlanner(wfCtx, reg, input, base, aggregatedToolResults, nextAttempt, seq, planner.TerminationReasonTimeBudget, deadline)
		}
		// Handle Await: publish await event, pause and wait for external input.
		if result.Await != nil {
			r.logger.Info(ctx, "PlanResult has Await, handling await")
			if ctrl == nil {
				return nil, errors.New("await not supported in inline runs")
			}
			// Publish paused event with a reason.
			reason := "await_external"
			if result.Await.Clarification != nil {
				reason = "await_clarification"
			}
			// If planner provided structured await info, emit a typed await event first.
			if result.Await.Clarification != nil {
				c := result.Await.Clarification
				r.publishHook(ctx, hooks.NewAwaitClarificationEvent(
					base.RunContext.RunID,
					base.Agent.ID(),
					base.RunContext.SessionID,
					c.ID,
					c.Question,
					c.MissingFields,
					c.RestrictToTool,
					c.ExampleInput,
				), seq)
			} else if result.Await.ExternalTools != nil {
				e := result.Await.ExternalTools
				items := make([]hooks.AwaitToolItem, 0, len(e.Items))
				for _, it := range e.Items {
					items = append(items, hooks.AwaitToolItem{
						ToolName:   it.Name,
						ToolCallID: it.ToolCallID,
						Payload:    it.Payload,
					})
				}
				r.publishHook(ctx, hooks.NewAwaitExternalToolsEvent(
					base.RunContext.RunID,
					base.Agent.ID(),
					base.RunContext.SessionID,
					e.ID,
					items,
				), seq)
			}
			r.publishHook(ctx, hooks.NewRunPausedEvent(
				base.RunContext.RunID,
				base.Agent.ID(),
				base.RunContext.SessionID,
				reason,
				"runtime",
				nil,
				nil,
			), seq)
			// Block until the appropriate provide signal arrives.
			if result.Await.Clarification != nil {
				ans, err := ctrl.WaitProvideClarification(ctx)
				if err != nil {
					return nil, err
				}
				// Validate await ID when provided
				if c := result.Await.Clarification; c != nil && c.ID != "" && ans.ID != "" && ans.ID != c.ID {
					return nil, errors.New("unexpected await ID for clarification")
				}
				// Append the answer as a user message and continue planning.
				if ans.Answer != "" {
					base.Messages = append(base.Messages, &model.Message{
						Role:  model.ConversationRoleUser,
						Parts: []model.Part{model.TextPart{Text: ans.Answer}},
					})
				}
				// Set running and emit resumed
				r.recordRunStatus(ctx, input, run.StatusRunning, map[string]any{
					"resumed_by": "clarification",
				})
				r.publishHook(
					ctx,
					hooks.NewRunResumedEvent(
						base.RunContext.RunID,
						base.Agent.ID(),
						base.RunContext.SessionID,
						"clarification_provided",
						ans.RunID,
						ans.Labels,
						1,
					),
					seq,
				)
				// Immediately PlanResume
				resumeCtx := base.RunContext
				resumeCtx.Attempt = nextAttempt
				nextAttempt++
				resumeReq := PlanActivityInput{
					AgentID:     base.Agent.ID(),
					RunID:       base.RunContext.RunID,
					Messages:    base.Messages,
					RunContext:  resumeCtx,
					ToolResults: nil,
				}
				var err2 error
				resOutput, err2 := r.runPlanActivity(wfCtx, reg.ResumeActivityName, resumeOpts, resumeReq, deadline)
				if err2 != nil {
					return nil, err2
				}
				if resOutput == nil || resOutput.Result == nil {
					return nil, fmt.Errorf("plan resume activity returned nil result after clarification")
				}
				result = resOutput.Result
				turnTranscript = resOutput.Transcript
				led = transcript.FromModelMessages(turnTranscript)
				continue
			} else if result.Await.ExternalTools != nil {
				// Record the assistant turn (including tool_use blocks) before
				// waiting for external tool results. This ensures that when the
				// planner resumes, base.Messages contains the assistant's tool_use
				// declarations that correspond to the tool_result blocks we'll add.
				e := result.Await.ExternalTools
				if len(e.Items) == 0 {
					return nil, errors.New("await_external_tools: no items in await")
				}
				awaitCalls := make([]planner.ToolRequest, 0, len(e.Items))
				expectedIDs := make(map[string]struct{}, len(e.Items))
				for _, it := range e.Items {
					if it.ToolCallID == "" {
						return nil, fmt.Errorf(
							"await_external_tools: missing tool_call_id for external tool %q",
							it.Name,
						)
					}
					if _, dup := expectedIDs[it.ToolCallID]; dup {
						return nil, fmt.Errorf(
							"await_external_tools: duplicate awaited tool_call_id %q",
							it.ToolCallID,
						)
					}
					expectedIDs[it.ToolCallID] = struct{}{}
					awaitCalls = append(awaitCalls, planner.ToolRequest{
						Name:       it.Name,
						ToolCallID: it.ToolCallID,
						Payload:    it.Payload,
					})
				}
				r.recordAssistantTurn(base, turnTranscript, awaitCalls, led)

				rs, err := ctrl.WaitProvideToolResults(ctx)
				if err != nil {
					return nil, err
				}
				// Validate await ID when provided
				if e.ID != "" && rs.ID != "" && rs.ID != e.ID {
					return nil, errors.New("unexpected await ID for external tools")
				}
				// Enforce strong contracts on external tool results: all results
				// must carry non-empty tool_call_id values that exactly match the
				// awaited tool_use IDs (no extras, no omissions, no duplicates).
				if len(rs.Results) == 0 {
					return nil, errors.New("await_external_tools: no tool results provided")
				}
				seen := make(map[string]struct{}, len(rs.Results))
				for _, tr := range rs.Results {
					if tr == nil {
						return nil, errors.New("await_external_tools: nil tool result")
					}
					if tr.ToolCallID == "" {
						return nil, fmt.Errorf(
							"await_external_tools: result for tool %q missing tool_call_id",
							tr.Name,
						)
					}
					if _, dup := seen[tr.ToolCallID]; dup {
						return nil, fmt.Errorf(
							"await_external_tools: duplicate result for tool_call_id %q",
							tr.ToolCallID,
						)
					}
					seen[tr.ToolCallID] = struct{}{}
				}
				if len(seen) != len(expectedIDs) {
					return nil, fmt.Errorf(
						"await_external_tools: tool result ids did not match awaited tool_use ids (awaited=%d, got=%d)",
						len(expectedIDs),
						len(seen),
					)
				}
				for id := range seen {
					if _, ok := expectedIDs[id]; !ok {
						return nil, fmt.Errorf(
							"await_external_tools: unexpected tool result for tool_call_id %q",
							id,
						)
					}
				}

				// Feed results into next PlanResume turn directly and record them
				// in the transcript so the provider sees a canonical tool_use →
				// tool_result handshake, consistent with internal tools.
				lastToolResults = rs.Results
				aggregatedToolResults = append(
					aggregatedToolResults,
					cloneToolResults(rs.Results)...,
				)
				r.appendUserToolResults(base, awaitCalls, lastToolResults, led)

				// Set running and emit resumed
				r.recordRunStatus(ctx, input, run.StatusRunning, map[string]any{
					"resumed_by": "tool_results",
					"results":    len(lastToolResults),
				})
				r.publishHook(
					ctx,
					hooks.NewRunResumedEvent(
						base.RunContext.RunID,
						base.Agent.ID(),
						base.RunContext.SessionID,
						"tool_results_provided",
						input.RunID,
						nil,
						0,
					),
					seq,
				)

				// Advance to PlanResume immediately without executing internal tools.
				resumeReq := r.buildNextResumeRequest(base, lastToolResults, &nextAttempt)
				resOutput, err2 := r.runPlanActivity(
					wfCtx,
					reg.ResumeActivityName,
					resumeOpts,
					resumeReq,
					deadline,
				)
				if err2 != nil {
					return nil, err2
				}
				if resOutput == nil || resOutput.Result == nil {
					return nil, fmt.Errorf("plan resume activity returned nil result after tool results")
				}
				result = resOutput.Result
				turnTranscript = resOutput.Transcript
				led = transcript.FromModelMessages(turnTranscript)
				continue
			}
		}

		r.logger.Info(ctx, "Checking result.ToolCalls", "len", len(result.ToolCalls))

		if len(result.ToolCalls) == 0 {
			r.logger.Info(ctx, "No tool calls, checking FinalResponse")
			if result.FinalResponse == nil {
				r.logger.Error(ctx, "ERROR - Neither tool calls nor final response!")
				// CRITICAL: This error will be visible in workflow failure logs
				return nil, fmt.Errorf("CRITICAL: planner returned neither tool calls nor final response - ToolCalls=%d, FinalResponse=%v, Await=%v", len(result.ToolCalls), result.FinalResponse != nil, result.Await != nil)
			}
			finalMsg := result.FinalResponse.Message
			if result.Streamed && agentMessageText(finalMsg) == "" {
				if text := transcriptText(turnTranscript); text != "" {
					finalMsg = newTextAgentMessage(model.ConversationRoleAssistant, text)
				}
			}
			if !result.Streamed {
				r.publishHook(
					ctx,
					hooks.NewAssistantMessageEvent(
						base.RunContext.RunID,
						base.Agent.ID(),
						base.RunContext.SessionID,
						agentMessageText(finalMsg),
						nil,
					),
					seq,
				)
			}
			for _, note := range result.Notes {
				r.publishHook(
					ctx,
					hooks.NewPlannerNoteEvent(
						base.RunContext.RunID,
						base.Agent.ID(),
						base.RunContext.SessionID,
						note.Text,
						note.Labels,
					),
					seq,
				)
			}
			notes := make([]*planner.PlannerAnnotation, len(result.Notes))
			for i := range result.Notes {
				notes[i] = &result.Notes[i]
			}
			return &RunOutput{
				AgentID:    base.Agent.ID(),
				RunID:      base.RunContext.RunID,
				Final:      finalMsg,
				ToolEvents: aggregatedToolResults,
				Notes:      notes,
				Usage:      &aggUsage,
			}, nil
		}

		if caps.RemainingToolCalls == 0 && caps.MaxToolCalls > 0 {
			// Tool cap exhausted: request a final tool-free response from the planner.
			return r.finalizeWithPlanner(wfCtx, reg, input, base, aggregatedToolResults, nextAttempt, seq, planner.TerminationReasonToolCap, deadline)
		}
		if !deadline.IsZero() && wfCtx.Now().After(deadline) {
			// Time budget exceeded after evaluating caps/policy
			return r.finalizeWithPlanner(
				wfCtx,
				reg,
				input,
				base,
				aggregatedToolResults,
				nextAttempt,
				seq,
				planner.TerminationReasonTimeBudget,
				deadline,
			)
		}
		// Start with candidate tool calls from the planner.
		candidates := result.ToolCalls

		r.logger.Info(ctx, "Workflow received tool calls from planner", "count", len(candidates))
		// Apply per-run overrides (restrict tool and tag filters) before policy decision.
		candidates = r.applyPerRunOverrides(ctx, input, candidates)
		// Apply runtime policy if configured.
		allowed, caps, err := r.applyRuntimePolicy(ctx, base, input, candidates, caps, seq, result.RetryHint)
		if err != nil {
			return nil, err
		}
		if len(allowed) == 0 {
			r.logger.Error(ctx, "ERROR - No tools allowed for execution after filtering", "candidates", len(result.ToolCalls))
			return nil, errors.New("no tools allowed for execution")
		}
		r.logger.Info(ctx, "Executing allowed tool calls", "count", len(allowed))
		if parentTracker != nil {
			ids := collectToolCallIDs(allowed)
			if len(ids) > 0 && parentTracker.registerDiscovered(ids) {
				r.publishHook(
					ctx,
					hooks.NewToolCallUpdatedEvent(
						base.RunContext.RunID,
						base.Agent.ID(),
						base.RunContext.SessionID,
						parentTracker.parentToolCallID,
						parentTracker.currentTotal(),
					),
					seq,
				)
				parentTracker.markUpdated()
			}
		}
		// Enforce per-turn and remaining caps and stamp metadata.
		allowed = r.capAllowedCalls(allowed, input, caps)
		allowed = r.prepareAllowedCallsMetadata(base, allowed, parentTracker)
		// Record assistant turn (thinking/text from transcript plus declared tool_use).
		r.recordAssistantTurn(base, turnTranscript, allowed, led)
		// Group calls by timeout and execute.
		grouped, timeouts := r.groupToolCallsByTimeout(allowed, input, toolOpts.Timeout)
		vals, err := r.executeGroupedToolCalls(
			wfCtx, reg, base, result.ExpectedChildren, seq, parentTracker, deadline,
			grouped, timeouts, toolOpts,
		)
		if err != nil {
			return nil, err
		}
		// Directly use pointer results for the planner input.
		lastToolResults = vals
		aggregatedToolResults = append(aggregatedToolResults, cloneToolResults(vals)...)
		// Decrement cap by the number of tool calls executed, not the number of results returned.
		// This ensures the cap is properly decremented even if some results are missing.
		caps.RemainingToolCalls = decrementCap(caps.RemainingToolCalls, len(allowed))
		// Append user tool_result message and update ledger in the same order as
		// the assistant tool_use declarations for this turn.
		r.appendUserToolResults(base, allowed, vals, led)
		if failures(vals) > 0 {
			caps.RemainingConsecutiveFailedToolCalls = decrementCap(
				caps.RemainingConsecutiveFailedToolCalls, failures(vals),
			)
			if caps.MaxConsecutiveFailedToolCalls > 0 && caps.RemainingConsecutiveFailedToolCalls <= 0 {
				// Consecutive failure cap exhausted: request finalization.
				return r.finalizeWithPlanner(wfCtx, reg, input, base, aggregatedToolResults, nextAttempt, seq, planner.TerminationReasonFailureCap, deadline)
			}
		} else if caps.MaxConsecutiveFailedToolCalls > 0 {
			caps.RemainingConsecutiveFailedToolCalls = caps.MaxConsecutiveFailedToolCalls
		}

		// Apply missing-fields policy if configured. The helper may finalize or pause/resume.
		if out, err := r.handleMissingFieldsPolicy(wfCtx, reg, input, base, vals, aggregatedToolResults, &nextAttempt, seq, ctrl); err != nil {
			return nil, err
		} else if out != nil {
			return out, nil
		}

		// Hard protection: If this turn executed agent-as-tool calls
		// and they produced zero child tool calls in total, do not
		// resume. Finalize immediately to avoid loops.
		if r.hardProtectionIfNeeded(ctx, base, vals, seq) {
			return r.finalizeWithPlanner(wfCtx, reg, input, base, aggregatedToolResults, nextAttempt, seq, planner.TerminationReasonFailureCap, deadline)
		}

		resumeReq := r.buildNextResumeRequest(base, lastToolResults, &nextAttempt)
		resOutput, rerr := r.runPlanActivity(wfCtx, reg.ResumeActivityName, resumeOpts, resumeReq, deadline)
		if rerr != nil {
			return nil, rerr
		}
		if resOutput == nil || resOutput.Result == nil {
			return nil, fmt.Errorf("plan activity returned nil result on resume")
		}
		result = resOutput.Result
		turnTranscript = resOutput.Transcript
		led = transcript.FromModelMessages(turnTranscript)
	}
}

// handleMissingFieldsPolicy inspects tool results for a RetryHint indicating missing
// required fields and applies the agent RunPolicy.OnMissingFields behavior:
//
//   - MissingFieldsFinalize: immediately finalize by requesting a tool-free final answer
//     from the planner. Returns a non-nil RunOutput to short-circuit the loop.
//   - MissingFieldsAwaitClarification: when durable (interrupt controller present), emit
//     an await_clarification event and pause the run. On resume, appends the user answer
//     as a message to the base PlanInput so the next turn can proceed. Returns handled=true.
//   - MissingFieldsResume (or unspecified): do nothing; the planner will see RetryHints
//     and may choose how to proceed. Returns handled=false.
//
// The function returns:
//   - out: non-nil only when finalization occurred
//   - handled: true when a pause/resume cycle was performed
//   - err: any error encountered while pausing/resuming
func (r *Runtime) handleMissingFieldsPolicy(
	wfCtx engine.WorkflowContext,
	reg AgentRegistration,
	input *RunInput,
	base *planner.PlanInput,
	results []*planner.ToolResult,
	allResults []*planner.ToolResult,
	nextAttempt *int,
	seq *turnSequencer,
	ctrl *interrupt.Controller,
) (*RunOutput, error) {
	if ctrl == nil || reg.Policy.OnMissingFields == "" {
		return nil, nil
	}
	ctx := wfCtx.Context()
	// Find first result with missing-fields hint and capture tool context.
	var (
		mf          *planner.RetryHint
		triggerTool tools.Ident
		triggerCall string
	)
	for _, tr := range results {
		if tr == nil || tr.RetryHint == nil {
			continue
		}
		if tr.RetryHint.Reason == planner.RetryReasonMissingFields || len(tr.RetryHint.MissingFields) > 0 {
			mf = tr.RetryHint
			triggerTool = tr.Name
			triggerCall = tr.ToolCallID
			break
		}
	}
	if mf == nil {
		return nil, nil
	}
	switch reg.Policy.OnMissingFields {
	case MissingFieldsFinalize:
		out, err := r.finalizeWithPlanner(wfCtx, reg, input, base, allResults, *nextAttempt, seq, planner.TerminationReasonFailureCap, time.Time{})
		return out, err
	case MissingFieldsAwaitClarification:
		// Generate deterministic await ID for correlation safety.
		awaitID := generateDeterministicAwaitID(base.RunContext.RunID, base.RunContext.TurnID, triggerTool, triggerCall)
		var restrict tools.Ident
		if mf.RestrictToTool {
			restrict = mf.Tool
		}
		r.publishHook(ctx, hooks.NewAwaitClarificationEvent(
			base.RunContext.RunID,
			base.Agent.ID(),
			base.RunContext.SessionID,
			awaitID,
			mf.ClarifyingQuestion,
			mf.MissingFields,
			restrict,
			mf.ExampleInput,
		), seq)
		r.publishHook(
			ctx,
			hooks.NewRunPausedEvent(
				base.RunContext.RunID,
				base.Agent.ID(),
				base.RunContext.SessionID,
				"await_clarification",
				"runtime",
				nil,
				nil,
			),
			seq,
		)
		ans, err := ctrl.WaitProvideClarification(ctx)
		if err != nil {
			return nil, err
		}
		// Validate correlation when ID is present on the answer.
		if ans.ID != "" && ans.ID != awaitID {
			return nil, fmt.Errorf("unexpected await ID for clarification")
		}
		if ans.Answer != "" {
			base.Messages = append(base.Messages, &model.Message{
				Role:  model.ConversationRoleUser,
				Parts: []model.Part{model.TextPart{Text: ans.Answer}},
			})
		}
		r.recordRunStatus(ctx, input, run.StatusRunning, map[string]any{
			"resumed_by": "clarification",
		})
		r.publishHook(ctx, hooks.NewRunResumedEvent(
			base.RunContext.RunID,
			base.Agent.ID(),
			base.RunContext.SessionID,
			"clarification_provided",
			input.RunID,
			ans.Labels,
			1,
		), seq)
		return nil, nil
	case MissingFieldsResume:
		return nil, nil
	default:
		return nil, nil
	}
}

// finalizeWithPlanner asks the planner for a tool-free final response and returns it as RunOutput.
func (r *Runtime) finalizeWithPlanner(
	wfCtx engine.WorkflowContext,
	reg AgentRegistration,
	input *RunInput,
	base *planner.PlanInput,
	allToolResults []*planner.ToolResult,
	nextAttempt int,
	seq *turnSequencer,
	reason planner.TerminationReason,
	hardDeadline time.Time,
) (*RunOutput, error) {
	if base == nil {
		return nil, errors.New("base plan input is required")
	}
	ctx := wfCtx.Context()
	// Transition to synthesizing phase while we obtain a final answer without
	// scheduling additional tools.
	r.publishHook(
		ctx,
		hooks.NewRunPhaseChangedEvent(
			base.RunContext.RunID,
			base.Agent.ID(),
			base.RunContext.SessionID,
			run.PhaseSynthesizing,
		),
		seq,
	)
	// Prepare a brief message to steer planners that incorporate system messages.
	var hint string
	switch reason {
	case planner.TerminationReasonTimeBudget:
		hint = "Time budget reached. Provide the best possible final answer now. Do not call any tools."
	case planner.TerminationReasonToolCap:
		hint = "Tool budget exhausted. Provide the best possible final answer now. Do not call any tools."
	case planner.TerminationReasonFailureCap:
		hint = "Too many tool failures. Provide the best possible final answer now. Do not call any tools."
	default:
		hint = "Provide the best possible final answer now. Do not call any tools."
	}
	messages := base.Messages
	if hint != "" {
		messages = append(messages, &model.Message{
			Role:  model.ConversationRoleSystem,
			Parts: []model.Part{model.TextPart{Text: hint}},
		})
	}
	resumeCtx := base.RunContext
	resumeCtx.Attempt = nextAttempt
	// Signal zero remaining duration for any prompt engineering that uses MaxDuration
	resumeCtx.MaxDuration = "0s"
	req := PlanActivityInput{
		AgentID:     base.Agent.ID(),
		RunID:       base.RunContext.RunID,
		Messages:    messages,
		RunContext:  resumeCtx,
		ToolResults: allToolResults,
		Finalize:    &planner.Termination{Reason: reason, Message: hint},
	}
	// Emit a pause/resume pair to indicate a finalization turn began.
	r.publishHook(
		ctx,
		hooks.NewRunPausedEvent(
			base.RunContext.RunID,
			base.Agent.ID(),
			base.RunContext.SessionID,
			"finalize",
			"runtime",
			map[string]string{"reason": string(reason)},
			nil,
		),
		seq,
	)
	r.recordRunStatus(ctx, input, run.StatusRunning, map[string]any{"resumed_by": "finalize"})
	// Use base.RunContext.RunID for resilience when input is nil.
	r.publishHook(
		ctx,
		hooks.NewRunResumedEvent(
			base.RunContext.RunID,
			base.Agent.ID(),
			base.RunContext.SessionID,
			"finalize",
			base.RunContext.RunID,
			nil,
			0,
		),
		seq,
	)

	// Human‑readable reason strings for error contexts when finalization fails.
	reasonText := func() string {
		switch reason {
		case planner.TerminationReasonTimeBudget:
			return "time budget exceeded"
		case planner.TerminationReasonToolCap:
			return "tool call cap exceeded"
		case planner.TerminationReasonFailureCap:
			return "consecutive failed tool call cap exceeded"
		default:
			return "finalization failed"
		}
	}()

	// Apply run-level Plan timeout override to Resume if present.
	resumeOpts := reg.ResumeActivityOptions
	if input != nil && input.Policy != nil && input.Policy.PlanTimeout > 0 {
		resumeOpts.Timeout = input.Policy.PlanTimeout
	}
	output, err := r.runPlanActivity(wfCtx, reg.ResumeActivityName, resumeOpts, req, hardDeadline)
	if err != nil {
		// Surface the termination reason prominently; include underlying error for observability.
		return nil, fmt.Errorf("%s: %w", reasonText, err)
	}
	if output == nil || output.Result == nil || output.Result.FinalResponse == nil {
		return nil, fmt.Errorf("%s", reasonText)
	}
	finalMsg := output.Result.FinalResponse.Message
	if output.Result.Streamed && agentMessageText(finalMsg) == "" {
		if text := transcriptText(output.Transcript); text != "" {
			finalMsg = newTextAgentMessage(model.ConversationRoleAssistant, text)
		}
	}
	if !output.Result.Streamed {
		r.publishHook(
			ctx,
			hooks.NewAssistantMessageEvent(
				base.RunContext.RunID,
				base.Agent.ID(),
				base.RunContext.SessionID,
				agentMessageText(finalMsg),
				nil,
			),
			seq,
		)
	}
	for _, note := range output.Result.Notes {
		r.publishHook(
			ctx,
			hooks.NewPlannerNoteEvent(
				base.RunContext.RunID,
				base.Agent.ID(),
				base.RunContext.SessionID,
				note.Text,
				note.Labels,
			),
			seq,
		)
	}
	notes := make([]*planner.PlannerAnnotation, len(output.Result.Notes))
	for i := range output.Result.Notes {
		notes[i] = &output.Result.Notes[i]
	}
	return &RunOutput{
		AgentID:    base.Agent.ID(),
		RunID:      base.RunContext.RunID,
		Final:      finalMsg,
		ToolEvents: allToolResults,
		Notes:      notes,
		// Usage aggregation continues to be recorded by the surrounding loop
	}, nil
}

func (r *Runtime) handleInterrupts(
	wfCtx engine.WorkflowContext,
	input *RunInput,
	base *planner.PlanInput,
	seq *turnSequencer,
	ctrl *interrupt.Controller,
	nextAttempt *int,
) error {
	if ctrl == nil {
		return nil
	}
	ctx := wfCtx.Context()
	for {
		req, ok := ctrl.PollPause()
		if !ok {
			break
		}
		r.recordRunStatus(ctx, input, run.StatusPaused, map[string]any{
			"reason": req.Reason,
		})
		r.publishHook(
			ctx,
			hooks.NewRunPausedEvent(
				input.RunID,
				input.AgentID,
				input.SessionID,
				req.Reason,
				req.RequestedBy,
				req.Labels,
				req.Metadata,
			),
			seq,
		)

		resumeReq, err := ctrl.WaitResume(ctx)
		if err != nil {
			return err
		}
		if len(resumeReq.Messages) > 0 {
			base.Messages = append(base.Messages, resumeReq.Messages...)
		}
		base.RunContext.Attempt = *nextAttempt
		*nextAttempt++
		r.recordRunStatus(ctx, input, run.StatusRunning, map[string]any{
			"resumed_by": resumeReq.RequestedBy,
		})
		r.publishHook(
			ctx,
			hooks.NewRunResumedEvent(
				input.RunID,
				input.AgentID,
				input.SessionID,
				resumeReq.Notes,
				resumeReq.RequestedBy,
				resumeReq.Labels,
				len(resumeReq.Messages),
			),
			seq,
		)
	}
	return nil
}

// executeToolCalls schedules tool activities in parallel and collects their results.
// Tools are launched asynchronously via ExecuteActivityAsync, then results are collected
// in order. This provides better performance for independent tool calls while maintaining
// deterministic result ordering. expectedChildren indicates how many child tools are expected
// to be discovered dynamically by the tools in this batch (0 if not tracked).
func collectToolCallIDs(calls []planner.ToolRequest) []string {
	ids := make([]string, 0, len(calls))
	for _, call := range calls {
		ids = append(ids, call.ToolCallID)
	}
	return ids
}

func (r *Runtime) executeToolCalls(
	wfCtx engine.WorkflowContext,
	activityName string, toolActOptions engine.ActivityOptions, runID string, agentID agent.Ident,
	runCtx *run.Context,
	calls []planner.ToolRequest,
	expectedChildren int,
	seq *turnSequencer,
	parentTracker *childTracker,
	hardDeadline time.Time,
) ([]*planner.ToolResult, error) {
	ctx := wfCtx.Context()
	eventRunID := runID
	eventAgentID := agentID
	sessionID := ""
	if runCtx != nil {
		sessionID = runCtx.SessionID
		if runCtx.ParentRunID != "" {
			eventRunID = runCtx.ParentRunID
		}
		if runCtx.ParentAgentID != "" {
			eventAgentID = runCtx.ParentAgentID
		}
	}

	// Decide per-call execution path (inline vs activity) by toolset.
	// Inline toolsets run synchronously within the workflow loop (agent-as-tool child workflows).
	// Others schedule activities and collect futures. Results are returned in call order.
	futures := make([]futureInfo, 0, len(calls))
	discoveredIDs := make([]string, 0, len(calls))
	inlineResults := make(map[string]*planner.ToolResult, len(calls))
	for i, call := range calls {
		if call.ToolCallID == "" {
			call.ToolCallID = generateDeterministicToolCallID(runID, call.TurnID, call.Name, i)
			calls[i] = call
		}
		if parentTracker != nil && call.ParentToolCallID == "" {
			call.ParentToolCallID = parentTracker.parentToolCallID
			calls[i] = call
		}
		if call.ParentToolCallID == "" && runCtx != nil && runCtx.ParentToolCallID != "" {
			call.ParentToolCallID = runCtx.ParentToolCallID
			calls[i] = call
		}

		spec, hasSpec := r.toolSpec(call.Name)
		if !hasSpec {
			return nil, fmt.Errorf("unknown tool %q", call.Name)
		}
		ts, hasTS := r.toolsets[spec.Toolset]

		// Prepare scheduled event (queue filled only for activity execution)
		queue := ""
		if hasTS && ts.TaskQueue != "" {
			queue = ts.TaskQueue
		}
		r.publishHook(
			ctx,
			hooks.NewToolCallScheduledEvent(
				eventRunID,
				eventAgentID,
				sessionID,
				call.Name,
				call.ToolCallID,
				call.Payload,
				queue,
				call.ParentToolCallID,
				expectedChildren,
			),
			seq,
		)

		// Inline path: agent-as-tool executes within workflow (starts child workflow inside Execute).
		if hasTS && ts.Inline {
			// Apply optional payload adapter before inline execution so agent-tools
			// see the same normalized payload shape as service-backed tools.
			raw := call.Payload
			if ts.PayloadAdapter != nil && len(raw) > 0 {
				meta := ToolCallMeta{
					RunID:            call.RunID,
					SessionID:        call.SessionID,
					TurnID:           call.TurnID,
					ToolCallID:       call.ToolCallID,
					ParentToolCallID: call.ParentToolCallID,
				}
				if adapted, err := ts.PayloadAdapter(ctx, meta, call.Name, raw); err == nil && len(adapted) > 0 {
					raw = adapted
				} else if err != nil {
					return nil, fmt.Errorf("inline payload adapter failed for %s: %w", call.Name, err)
				}
			}
			if len(raw) > 0 {
				call.Payload = raw
				calls[i].Payload = raw
			}
			start := wfCtx.Now()
			// Attach workflow context so agent-tool executor can start a child workflow.
			ctxInline := engine.WithWorkflowContext(ctx, wfCtx)
			ctxInline = withFinalizerInvokerFactory(ctxInline, &finalizerInvokerFactory{
				runtime:         r,
				wfCtx:           wfCtx,
				activityName:    activityName,
				activityOptions: toolActOptions,
				agentID:         agentID,
			})
			result, err := ts.Execute(ctxInline, &call)
			if err != nil {
				return nil, fmt.Errorf("inline tool %q failed: %w", call.Name, err)
			}
			if result == nil {
				return nil, fmt.Errorf("inline tool %q returned nil result", call.Name)
			}
			// Publish result event for observability parity with activities.
			duration := wfCtx.Now().Sub(start)
			var toolErr *planner.ToolError
			if result.Error != nil {
				toolErr = result.Error
			}
			r.publishHook(
				ctx,
				hooks.NewToolResultReceivedEvent(
					eventRunID,
					eventAgentID,
					sessionID,
					call.Name,
					call.ToolCallID,
					call.ParentToolCallID,
					result.Result,
					result.Sidecar,
					duration,
					result.Telemetry,
					toolErr,
				),
				seq,
			)
			inlineResults[call.ToolCallID] = result
			if parentTracker != nil {
				discoveredIDs = append(discoveredIDs, call.ToolCallID)
			}
			continue
		}

		// Activity path (service-backed tools).
		req := engine.ActivityRequest{
			Name: activityName,
			Input: ToolInput{
				AgentID:          agentID,
				RunID:            runID,
				ToolsetName:      spec.Toolset,
				ToolName:         call.Name,
				ToolCallID:       call.ToolCallID,
				Payload:          call.Payload,
				SessionID:        call.SessionID,
				TurnID:           call.TurnID,
				ParentToolCallID: call.ParentToolCallID,
			},
		}
		// Apply strong-contract execute_tool options first
		if toolActOptions.Queue != "" {
			req.Queue = toolActOptions.Queue
		}
		// Apply timeout: start with configured, then cap to remaining time to hard deadline.
		timeout := toolActOptions.Timeout
		if !hardDeadline.IsZero() {
			now := wfCtx.Now()
			if rem := hardDeadline.Sub(now); rem > 0 {
				if timeout == 0 || timeout > rem {
					timeout = rem
				}
			}
		}
		if timeout > 0 {
			req.Timeout = timeout
		}
		if !isZeroRetryPolicy(toolActOptions.RetryPolicy) {
			req.RetryPolicy = toolActOptions.RetryPolicy
		}
		// If no queue specified via options, allow explicit toolset TaskQueue to set it
		// for service-backed toolsets only. Agent-as-tool (Inline) must execute on the
		// parent agent's queue so the provider is started as a child workflow by the
		// ExecuteTool activity handler.
		if req.Queue == "" && hasTS && !ts.Inline && ts.TaskQueue != "" {
			req.Queue = ts.TaskQueue
		}
		future, err := wfCtx.ExecuteActivityAsync(ctx, req)
		if err != nil {
			return nil, fmt.Errorf("failed to schedule tool %q: %w", call.Name, err)
		}
		futures = append(futures, futureInfo{
			future:    future,
			call:      call,
			startTime: wfCtx.Now(),
		})
		if parentTracker != nil {
			discoveredIDs = append(discoveredIDs, call.ToolCallID)
		}
	}

	if parentTracker != nil && parentTracker.registerDiscovered(discoveredIDs) && parentTracker.needsUpdate() {
		r.publishHook(
			ctx,
			hooks.NewToolCallUpdatedEvent(
				eventRunID,
				eventAgentID,
				sessionID,
				parentTracker.parentToolCallID,
				parentTracker.currentTotal(),
			),
			seq,
		)
		parentTracker.markUpdated()
	}

	// Collect all results in order
	results := make([]*planner.ToolResult, 0, len(calls))
	// Collect activity results
	activityByID := make(map[string]*planner.ToolResult, len(futures))
	for _, info := range futures {
		var out ToolOutput
		if err := info.future.Get(ctx, &out); err != nil {
			return nil, fmt.Errorf("tool %q failed: %w", info.call.Name, err)
		}

		duration := wfCtx.Now().Sub(info.startTime)
		// Decode tool result. If decoding fails (mismatch with generated result
		// type), do not abort the workflow. Forward the raw payload to preserve
		// observability and allow clients to render best‑effort data.
		var decoded any
		if len(out.Payload) > 0 {
			if v, decErr := r.unmarshalToolValue(ctx, info.call.Name, out.Payload, false); decErr == nil {
				decoded = v
			} else {
				decoded = out.Payload
			}
		}

		toolRes := &planner.ToolResult{
			Name:       info.call.Name,
			Result:     decoded,
			Sidecar:    out.Sidecar,
			ToolCallID: info.call.ToolCallID,
			Telemetry:  out.Telemetry,
		}
		var toolErr *planner.ToolError
		if out.Error != "" {
			toolErr = planner.NewToolError(out.Error)
			toolRes.Error = toolErr
		}
		if out.RetryHint != nil {
			toolRes.RetryHint = out.RetryHint
		}

		parentID := info.call.ParentToolCallID
		if parentID == "" && runCtx != nil {
			parentID = runCtx.ParentToolCallID
		}
		r.publishHook(
			ctx,
			hooks.NewToolResultReceivedEvent(
				eventRunID,
				eventAgentID,
				sessionID,
				info.call.Name,
				info.call.ToolCallID,
				parentID,
				decoded,
				out.Sidecar,
				duration,
				out.Telemetry,
				toolErr,
			),
			seq,
		)

		activityByID[info.call.ToolCallID] = toolRes
	}

	// Merge results following original call order (inline or activity)
	for _, call := range calls {
		if ar, ok := activityByID[call.ToolCallID]; ok {
			results = append(results, ar)
			continue
		}
		if ir, ok := inlineResults[call.ToolCallID]; ok {
			results = append(results, ir)
			continue
		}
		// Should not happen; defensive: keep order consistent even if missing.
		r.logWarn(ctx, "missing tool result for call", fmt.Errorf("no result"), "tool", call.Name, "tool_call_id", call.ToolCallID)
	}

	return results, nil
}

// runPlanActivity schedules a plan/resume activity with the configured options.
func (r *Runtime) runPlanActivity(
	wfCtx engine.WorkflowContext, activityName string, options engine.ActivityOptions, input PlanActivityInput, hardDeadline time.Time,
) (*PlanActivityOutput, error) {
	if activityName == "" {
		return nil, errors.New("plan activity not registered")
	}
	var out PlanActivityOutput
	req := engine.ActivityRequest{
		Name:  activityName,
		Input: input,
	}
	if options.Queue != "" {
		req.Queue = options.Queue
	}
	// Apply timeout: start with configured, then cap to remaining time to hard deadline.
	timeout := options.Timeout
	if !hardDeadline.IsZero() {
		now := wfCtx.Now()
		if rem := hardDeadline.Sub(now); rem > 0 {
			if timeout == 0 || timeout > rem {
				timeout = rem
			}
		}
	}
	if timeout > 0 {
		req.Timeout = timeout
	}
	if !isZeroRetryPolicy(options.RetryPolicy) {
		req.RetryPolicy = options.RetryPolicy
	}
	if err := wfCtx.ExecuteActivity(wfCtx.Context(), req, &out); err != nil {
		return nil, err
	}
	if out.Result == nil {
		return nil, fmt.Errorf("CRITICAL: runPlanActivity received nil PlanResult")
	}
	if len(out.Result.ToolCalls) == 0 && out.Result.FinalResponse == nil && out.Result.Await == nil {
		return nil, fmt.Errorf("CRITICAL: runPlanActivity received PlanResult with no ToolCalls, FinalResponse, or Await")
	}
	r.logger.Info(wfCtx.Context(), "runPlanActivity received PlanResult", "tool_calls", len(out.Result.ToolCalls), "final_response", out.Result.FinalResponse != nil, "await", out.Result.Await != nil)
	return &out, nil
}

// applyPerRunOverrides filters candidate tool calls using per-run overrides:
// RestrictToTool and tag allow/deny lists. Returns the filtered slice.
func (r *Runtime) applyPerRunOverrides(
	ctx context.Context,
	input *RunInput,
	candidates []planner.ToolRequest,
) []planner.ToolRequest {
	if input == nil || input.Policy == nil || len(candidates) == 0 {
		return candidates
	}
	ov := input.Policy
	if ov.RestrictToTool == "" && len(ov.AllowedTags) == 0 && len(ov.DeniedTags) == 0 {
		return candidates
	}
	r.logger.Info(ctx, "Applying per-run policy overrides", "restrict_to_tool", ov.RestrictToTool, "allowed_tags", ov.AllowedTags, "denied_tags", ov.DeniedTags)
	metas := r.toolMetadata(candidates)
	filtered := make([]planner.ToolRequest, 0, len(candidates))
	for i, call := range candidates {
		if ov.RestrictToTool != "" && call.Name != ov.RestrictToTool {
			r.logger.Info(ctx, "Tool filtered by RestrictToTool", "tool", call.Name)
			continue
		}
		ok := true
		if len(ov.AllowedTags) > 0 || len(ov.DeniedTags) > 0 {
			tags := metas[i].Tags
			if len(ov.AllowedTags) > 0 && !hasIntersection(tags, ov.AllowedTags) {
				r.logger.Info(ctx, "Tool filtered by AllowedTags", "tool", call.Name, "tags", tags, "required", ov.AllowedTags)
				ok = false
			}
			if ok && len(ov.DeniedTags) > 0 && hasIntersection(tags, ov.DeniedTags) {
				r.logger.Info(ctx, "Tool filtered by DeniedTags", "tool", call.Name, "tags", tags, "denied", ov.DeniedTags)
				ok = false
			}
		}
		if ok {
			filtered = append(filtered, call)
		}
	}
	r.logger.Info(ctx, "After per-run policy filtering", "candidates", len(filtered))
	return filtered
}

// applyRuntimePolicy applies the runtime policy (if configured) to the provided
// candidates, returning the allowed set and updated caps. It also records and
// publishes the policy decision.
func (r *Runtime) applyRuntimePolicy(
	ctx context.Context,
	base *planner.PlanInput,
	input *RunInput,
	candidates []planner.ToolRequest,
	caps policy.CapsState,
	seq *turnSequencer,
	retry *planner.RetryHint,
) ([]planner.ToolRequest, policy.CapsState, error) {
	if r.Policy == nil {
		return candidates, caps, nil
	}
	r.logger.Info(ctx, "Applying runtime policy decision")
	decision, err := r.Policy.Decide(ctx, policy.Input{
		RunContext:    base.RunContext,
		Tools:         r.toolMetadata(candidates),
		RetryHint:     toPolicyRetryHint(retry),
		RemainingCaps: caps,
		Requested:     toolHandles(candidates),
		Labels:        base.RunContext.Labels,
	})
	if err != nil {
		return nil, caps, err
	}
	if len(decision.Labels) > 0 {
		base.RunContext.Labels = mergeLabels(base.RunContext.Labels, decision.Labels)
		input.Labels = mergeLabels(input.Labels, decision.Labels)
	}
	if decision.DisableTools {
		return nil, caps, errors.New("tool execution disabled by policy")
	}
	allowed := candidates
	if len(decision.AllowedTools) > 0 {
		allowed = filterToolCalls(allowed, decision.AllowedTools)
	}
	caps = mergeCaps(caps, decision.Caps)
	r.recordPolicyDecision(ctx, input, decision)
	r.publishHook(
		ctx,
		hooks.NewPolicyDecisionEvent(
			base.RunContext.RunID,
			base.Agent.ID(),
			base.RunContext.SessionID,
			decision.AllowedTools,
			caps,
			cloneLabels(decision.Labels),
			cloneMetadata(decision.Metadata),
		),
		seq,
	)
	return allowed, caps, nil
}

// capAllowedCalls applies per-turn and remaining caps to the allowed set.
func (r *Runtime) capAllowedCalls(
	allowed []planner.ToolRequest,
	input *RunInput,
	caps policy.CapsState,
) []planner.ToolRequest {
	if input != nil && input.Policy != nil && input.Policy.PerTurnMaxToolCalls > 0 && len(allowed) > input.Policy.PerTurnMaxToolCalls {
		allowed = allowed[:input.Policy.PerTurnMaxToolCalls]
	}
	if caps.MaxToolCalls > 0 && caps.RemainingToolCalls < len(allowed) {
		allowed = allowed[:caps.RemainingToolCalls]
	}
	return allowed
}

// prepareAllowedCallsMetadata stamps run/session/turn IDs and deterministic tool
// call IDs on allowed calls. It also fills parentToolCallID when tracking children.
func (r *Runtime) prepareAllowedCallsMetadata(
	base *planner.PlanInput,
	allowed []planner.ToolRequest,
	parentTracker *childTracker,
) []planner.ToolRequest {
	for i := range allowed {
		if allowed[i].RunID == "" {
			allowed[i].RunID = base.RunContext.RunID
		}
		if allowed[i].AgentID == "" && base.Agent != nil {
			allowed[i].AgentID = base.Agent.ID()
		}
		if allowed[i].SessionID == "" {
			allowed[i].SessionID = base.RunContext.SessionID
		}
		if allowed[i].TurnID == "" {
			allowed[i].TurnID = base.RunContext.TurnID
		}
		if allowed[i].ToolCallID == "" {
			allowed[i].ToolCallID = generateDeterministicToolCallID(
				base.RunContext.RunID, base.RunContext.TurnID, allowed[i].Name, i,
			)
		}
		if parentTracker != nil && allowed[i].ParentToolCallID == "" {
			allowed[i].ParentToolCallID = parentTracker.parentToolCallID
		}
	}
	return allowed
}

// recordAssistantTurn merges streamed transcript parts with the declared tool calls
// and appends the resulting assistant messages to the conversation state.
func (r *Runtime) recordAssistantTurn(
	base *planner.PlanInput,
	transcriptMsgs []*model.Message,
	allowed []planner.ToolRequest,
	led *transcript.Ledger,
) {
	if led == nil {
		led = transcript.NewLedger()
	}
	if len(transcriptMsgs) == 0 && len(allowed) == 0 {
		return
	}
	for _, call := range allowed {
		led.DeclareToolUse(call.ToolCallID, string(call.Name), call.Payload)
	}
	// Flush a single assistant message capturing the full turn (thinking/text
	// plus all tool_use blocks) so the next user message can correlate to the
	// complete set of tool_use IDs.
	led.FlushAssistant()
	messages := cloneMessages(transcriptMsgs)
	target := findAssistantMessage(messages)
	if target == nil {
		target = &model.Message{Role: model.ConversationRoleAssistant}
		messages = append(messages, target)
	}
	for _, call := range allowed {
		target.Parts = append(target.Parts, model.ToolUsePart{
			ID:    call.ToolCallID,
			Name:  string(call.Name),
			Input: call.Payload,
		})
	}
	base.Messages = append(base.Messages, messages...)
}

func findAssistantMessage(msgs []*model.Message) *model.Message {
	for i := len(msgs) - 1; i >= 0; i-- {
		if msgs[i] != nil && msgs[i].Role == model.ConversationRoleAssistant {
			return msgs[i]
		}
	}
	return nil
}

func cloneMessages(msgs []*model.Message) []*model.Message {
	if len(msgs) == 0 {
		return nil
	}
	out := make([]*model.Message, 0, len(msgs))
	for _, msg := range msgs {
		if msg == nil {
			continue
		}
		parts := make([]model.Part, len(msg.Parts))
		copy(parts, msg.Parts)
		out = append(out, &model.Message{
			Role:  msg.Role,
			Parts: parts,
			Meta:  cloneMetadata(msg.Meta),
		})
	}
	return out
}

// groupToolCallsByTimeout buckets calls by per-tool timeout override (with '*' prefix support)
// or falls back to the default timeout when no per-tool override applies.
func (r *Runtime) groupToolCallsByTimeout(
	allowed []planner.ToolRequest,
	input *RunInput,
	defaultTimeout time.Duration,
) ([][]planner.ToolRequest, []time.Duration) {
	var grouped [][]planner.ToolRequest
	var timeouts []time.Duration
	if input != nil && input.Policy != nil && len(input.Policy.PerToolTimeout) > 0 {
		buckets := make(map[time.Duration][]planner.ToolRequest)
		resolve := func(name tools.Ident) (time.Duration, bool) {
			for k, v := range input.Policy.PerToolTimeout {
				kn := string(k)
				n := string(name)
				if strings.HasSuffix(kn, "*") {
					prefix := strings.TrimSuffix(kn, "*")
					if strings.HasPrefix(n, prefix) {
						return v, true
					}
				} else if kn == n {
					return v, true
				}
			}
			return 0, false
		}
		for _, call := range allowed {
			if to, ok := resolve(call.Name); ok && to > 0 {
				buckets[to] = append(buckets[to], call)
			} else {
				buckets[defaultTimeout] = append(buckets[defaultTimeout], call)
			}
		}
		for to, calls := range buckets {
			grouped = append(grouped, calls)
			timeouts = append(timeouts, to)
		}
	} else {
		grouped = [][]planner.ToolRequest{allowed}
		timeouts = []time.Duration{defaultTimeout}
	}
	return grouped, timeouts
}

// executeGroupedToolCalls runs groups of tool calls with their respective timeouts
// and returns all results in the original group order.
func (r *Runtime) executeGroupedToolCalls(
	wfCtx engine.WorkflowContext,
	reg AgentRegistration,
	base *planner.PlanInput,
	expectedChildren int,
	seq *turnSequencer,
	parentTracker *childTracker,
	deadline time.Time,
	grouped [][]planner.ToolRequest,
	timeouts []time.Duration,
	toolOpts engine.ActivityOptions,
) ([]*planner.ToolResult, error) {
	var out []*planner.ToolResult
	for i := range grouped {
		opt := toolOpts
		if timeouts[i] > 0 {
			opt.Timeout = timeouts[i]
		}
		sub, err := r.executeToolCalls(
			wfCtx, reg.ExecuteToolActivity, opt, base.RunContext.RunID, base.Agent.ID(),
			&base.RunContext, grouped[i], expectedChildren, seq, parentTracker, deadline,
		)
		if err != nil {
			return nil, err
		}
		out = append(out, sub...)
	}
	return out, nil
}

// appendUserToolResults appends a user message with tool_result blocks for the
// executed tools and updates the ledger. Tool results are ordered to match the
// assistant tool_use IDs from the allowed calls slice so that provider
// handshakes remain deterministic regardless of execution timing.
func (r *Runtime) appendUserToolResults(
	base *planner.PlanInput,
	allowed []planner.ToolRequest,
	vals []*planner.ToolResult,
	led *transcript.Ledger,
) {
	if len(vals) == 0 {
		return
	}
	// Index results by ToolCallID for quick lookup.
	resultsByID := make(map[string]*planner.ToolResult, len(vals))
	for _, tr := range vals {
		if tr == nil || tr.ToolCallID == "" {
			continue
		}
		resultsByID[tr.ToolCallID] = tr
	}
	// Order results by allowed tool call IDs so tool_result blocks follow the
	// assistant tool_use declaration order.
	parts := make([]model.Part, 0, len(resultsByID))
	specs := make([]transcript.ToolResultSpec, 0, len(resultsByID))
	for _, call := range allowed {
		tr, ok := resultsByID[call.ToolCallID]
		if !ok || tr == nil || tr.ToolCallID == "" {
			continue
		}
		part := model.ToolResultPart{
			ToolUseID: tr.ToolCallID,
			Content:   tr.Result,
			IsError:   tr.Error != nil,
		}
		parts = append(parts, part)
		specs = append(specs, transcript.ToolResultSpec{
			ToolUseID: tr.ToolCallID,
			Content:   tr.Result,
			IsError:   tr.Error != nil,
		})
	}
	if len(parts) == 0 {
		return
	}
	base.Messages = append(base.Messages, &model.Message{
		Role:  model.ConversationRoleUser,
		Parts: parts,
	})
	led.AppendUserToolResults(specs)
}

// hardProtectionIfNeeded emits a protection event and signals finalization when
// agent-as-tool calls produced no child tool calls.
func (r *Runtime) hardProtectionIfNeeded(
	ctx context.Context,
	base *planner.PlanInput,
	vals []*planner.ToolResult,
	seq *turnSequencer,
) bool {
	var agentToolCount int
	var totalChildren int
	toolNames := make([]tools.Ident, 0, len(vals))
	for _, tr := range vals {
		if spec, ok := r.toolSpec(tr.Name); ok && spec.IsAgentTool {
			agentToolCount++
			toolNames = append(toolNames, tr.Name)
			if tr.ChildrenCount > 0 {
				totalChildren += tr.ChildrenCount
			}
		}
	}
	if agentToolCount > 0 && totalChildren == 0 {
		r.publishHook(
			ctx,
			hooks.NewHardProtectionEvent(
				base.RunContext.RunID,
				base.Agent.ID(),
				base.RunContext.SessionID,
				"agent_tool_no_children",
				agentToolCount,
				totalChildren,
				toolNames,
			),
			seq,
		)
		return true
	}
	return false
}

// buildNextResumeRequest converts the base plan input into provider-ready
// messages, validates constraints for thinking-enabled paths, and builds the
// next PlanActivityInput.
func (r *Runtime) buildNextResumeRequest(
	base *planner.PlanInput,
	lastToolResults []*planner.ToolResult,
	nextAttempt *int,
) PlanActivityInput {
	resumeCtx := base.RunContext
	resumeCtx.Attempt = *nextAttempt
	*nextAttempt++
	plannerMsgs := cloneMessages(base.Messages)
	if err := transcript.ValidateBedrock(plannerMsgs, false); err != nil {
		// Fail fast rather than sending an invalid sequence to the provider.
		// This surfaces a clear runtime/ledger bug to the caller.
		panic(fmt.Sprintf("invalid Bedrock transcript for run %s: %v", base.RunContext.RunID, err))
	}
	return PlanActivityInput{
		AgentID:     base.Agent.ID(),
		RunID:       base.RunContext.RunID,
		Messages:    plannerMsgs,
		RunContext:  resumeCtx,
		ToolResults: lastToolResults,
	}
}

// recordRunStatus upserts run metadata to the store if configured.
func (r *Runtime) recordRunStatus(ctx context.Context, input *RunInput, status run.Status, meta map[string]any) {
	if r.RunStore == nil {
		return
	}
	rec := run.Record{
		AgentID:   input.AgentID,
		RunID:     input.RunID,
		SessionID: input.SessionID,
		TurnID:    input.TurnID,
		Status:    status,
		StartedAt: time.Now(),
		UpdatedAt: time.Now(),
		Labels:    cloneLabels(input.Labels),
		Metadata:  meta,
	}
	if err := r.RunStore.Upsert(ctx, rec); err != nil {
		r.logWarn(ctx, "run record upsert failed", err)
	}
}

func (r *Runtime) recordPolicyDecision(ctx context.Context, input *RunInput, decision policy.Decision) {
	if r.RunStore == nil {
		return
	}
	rec, err := r.RunStore.Load(ctx, input.RunID)
	if err != nil {
		r.logWarn(ctx, "run record load failed", err, "run_id", input.RunID)
		return
	}
	now := time.Now()
	if rec.RunID == "" {
		rec.AgentID = input.AgentID
		rec.RunID = input.RunID
		rec.SessionID = input.SessionID
		rec.TurnID = input.TurnID
		rec.StartedAt = now
	}
	if rec.StartedAt.IsZero() {
		rec.StartedAt = now
	}
	rec.AgentID = input.AgentID
	rec.SessionID = input.SessionID
	rec.TurnID = input.TurnID
	rec.Status = run.StatusRunning
	rec.UpdatedAt = now
	rec.Labels = mergeLabels(rec.Labels, input.Labels)

	entry := map[string]any{
		"caps":      decision.Caps,
		"timestamp": now.UTC(),
	}
	if len(decision.AllowedTools) > 0 {
		entry["allowed_tools"] = decision.AllowedTools
	}
	if len(decision.Labels) > 0 {
		entry["labels"] = cloneLabels(decision.Labels)
	}
	if len(decision.Metadata) > 0 {
		entry["metadata"] = cloneMetadata(decision.Metadata)
	}
	if decision.DisableTools {
		entry["disable_tools"] = true
	}

	meta := cloneMetadata(rec.Metadata)
	meta = appendPolicyDecisionMetadata(meta, entry)
	rec.Metadata = meta

	if err := r.RunStore.Upsert(ctx, rec); err != nil {
		r.logWarn(ctx, "policy decision upsert failed", err)
	}
}

// registerUsageAggregator subscribes to Usage events for a specific run/agent and
// aggregates token usage into the provided accumulator. Returns a subscription
// handle that should be closed by the caller. If the bus is nil or registration
// fails, returns nil.
func (r *Runtime) registerUsageAggregator(
	ctx context.Context,
	runID string,
	agentID string,
	agg *model.TokenUsage,
) hooks.Subscription {
	if r.Bus == nil {
		return nil
	}
	sub, err := r.Bus.Register(hooks.SubscriberFunc(func(c context.Context, evt hooks.Event) error {
		if evt.RunID() != runID || evt.AgentID() != agentID {
			return nil
		}
		if u, ok := evt.(*hooks.UsageEvent); ok {
			*agg = model.TokenUsage{
				InputTokens:  agg.InputTokens + u.InputTokens,
				OutputTokens: agg.OutputTokens + u.OutputTokens,
				TotalTokens:  agg.TotalTokens + u.TotalTokens,
			}
		}
		return nil
	}))
	if err != nil {
		r.logWarn(ctx, "usage subscriber register failed", err)
		return nil
	}
	return sub
}

// memoryReader loads the run snapshot from the memory store and wraps it in a Reader.
func (r *Runtime) memoryReader(ctx context.Context, agentID, runID string) memory.Reader {
	if r.Memory == nil {
		return emptyMemoryReader{}
	}
	snapshot, err := r.Memory.LoadRun(ctx, agentID, runID)
	if err != nil {
		return emptyMemoryReader{}
	}
	return newMemoryReader(snapshot.Events)
}

// generateRunID creates a unique run identifier by combining the agent ID and a UUID.
func generateRunID(agentID string) string {
	prefix := strings.ReplaceAll(agentID, ".", "-")
	return fmt.Sprintf("%s-%s", prefix, uuid.NewString())
}

// nextSeq increments and returns the next sequence number for this turn.
func (t *turnSequencer) nextSeq() int {
	seq := t.sequence
	t.sequence++
	return seq
}
