package runtime

import (
	"context"
	"errors"
	"fmt"
	"strings"
	"time"

	"github.com/google/uuid"
	"goa.design/goa-ai/runtime/agent/engine"
	"goa.design/goa-ai/runtime/agent/hooks"
	"goa.design/goa-ai/runtime/agent/interrupt"
	"goa.design/goa-ai/runtime/agent/memory"
	"goa.design/goa-ai/runtime/agent/model"
	"goa.design/goa-ai/runtime/agent/planner"
	"goa.design/goa-ai/runtime/agent/policy"
	"goa.design/goa-ai/runtime/agent/run"
	"goa.design/goa-ai/runtime/agent/telemetry"
	"goa.design/goa-ai/runtime/agent/toolerrors"
	"goa.design/goa-ai/runtime/agent/tools"
)

type (
	// futureInfo bundles a Future with its associated tool call metadata for parallel execution.
	// When tools are launched asynchronously via ExecuteActivityAsync, we need to track the
	// future handle alongside the original call details and start time so we can correlate
	// results and measure duration when collecting completed activities.
	futureInfo struct {
		// future is the engine Future returned by ExecuteActivityAsync for this tool call.
		future engine.Future
		// call is the original tool request that was submitted for execution.
		call planner.ToolRequest
		// startTime records when the activity was scheduled, used to calculate tool duration.
		startTime time.Time
	}

	// turnSequencer tracks the turn ID and monotonic sequence counter for event ordering.
	// Each run maintains its own sequencer to ensure deterministic event ordering within
	// a conversational turn.
	turnSequencer struct {
		turnID   string
		sequence int
	}

	// childTracker tracks dynamically discovered child tool calls for a parent tool
	// (agent-as-tool pattern). As the planner discovers new child tools across iterations,
	// the tracker maintains the unique set of discovered IDs and triggers update events
	// when the count increases. This enables UI progress tracking ("3 of 5 complete").
	//
	// NOTE: This infrastructure is currently unused and reserved for future implementation
	// of nested agent-as-tool workflows where tools run their own internal planning loops
	// and dynamically discover child tools across multiple iterations. The struct and
	// methods are defined now to support future codegen and maintain API stability.
	childTracker struct {
		// parentToolCallID identifies the parent tool (usually an agent-as-tool invocation).
		parentToolCallID string
		// discovered maps tool call IDs to struct{} for efficient membership checking.
		// The map size is the current expected children total.
		discovered map[string]struct{}
		// lastExpectedTotal is the count last reported via ToolCallUpdatedEvent.
		// We only emit update events when len(discovered) > lastExpectedTotal.
		lastExpectedTotal int
	}
)

// ExecuteWorkflow is the main entry point for the generated workflow handler.
//
// Advanced & generated integration
//   - Intended to be invoked by code generated by goa-ai during agent
//     registration, or by advanced users writing custom engine adapters.
//   - Normal applications should prefer the high‑level AgentClient API
//     (Runtime.Client(...).Run/Start) rather than calling this directly.
//
// It executes the agent's plan/tool loop using the configured planner, policy,
// and runtime hooks. Returns the final agent output or an error if the workflow
// fails. Generated code calls this from the workflow handler registered with
// the engine.
func (r *Runtime) ExecuteWorkflow(wfCtx engine.WorkflowContext, input *RunInput) (*RunOutput, error) {
	if r.logger != nil {
		r.logger.Info(wfCtx.Context(), "ExecuteWorkflow called", "agent_id", input.AgentID, "run_id", input.RunID)
	}
	if input.AgentID == "" {
		return nil, errors.New("agent id is required")
	}
	defer r.storeWorkflowHandle(input.RunID, nil)
	reg, ok := r.agentByID(input.AgentID)
	if !ok {
		return nil, fmt.Errorf("agent %q is not registered", input.AgentID)
	}
	r.logger.Info(wfCtx.Context(), "Agent found, executing plan activity", "agent_id", input.AgentID)
	ctrl := interrupt.NewController(wfCtx)
	reader := r.memoryReader(wfCtx.Context(), input.AgentID, input.RunID)
	agentCtx := newAgentContext(agentContextOptions{
		runtime: r,
		agentID: input.AgentID,
		runID:   input.RunID,
		memory:  reader,
		turnID:  input.TurnID,
	})
	runCtx := run.Context{
		RunID:     input.RunID,
		SessionID: input.SessionID,
		TurnID:    input.TurnID,
		Attempt:   1,
		Labels:    input.Labels,
	}
	// Initialize turn sequencer for event ordering if TurnID is provided
	var seq *turnSequencer
	if input.TurnID != "" {
		seq = &turnSequencer{
			turnID: input.TurnID,
		}
	}
	r.publishHook(wfCtx.Context(), hooks.NewRunStartedEvent(input.RunID, input.AgentID, runCtx, *input), seq)
	r.recordRunStatus(wfCtx.Context(), input, run.StatusRunning, nil)
	defer r.publishHook(wfCtx.Context(), hooks.NewRunCompletedEvent(input.RunID, input.AgentID, "success", nil), seq)

	planInput := planner.PlanInput{
		Messages:   input.Messages,
		RunContext: runCtx,
		Agent:      agentCtx,
		Events:     newPlannerEvents(r, input.AgentID, input.RunID),
	}
	startReq := PlanActivityInput{
		AgentID:    input.AgentID,
		RunID:      input.RunID,
		Messages:   planInput.Messages,
		RunContext: planInput.RunContext,
	}
	result, err := r.runPlanActivity(wfCtx, reg.PlanActivityName, reg.PlanActivityOptions, startReq)
	if err != nil {
		r.logger.Error(wfCtx.Context(), "Plan activity failed", "error", err)
		r.recordRunStatus(wfCtx.Context(), input, run.StatusFailed, map[string]any{
			"error": err.Error(),
		})
		return nil, err
	}
	if result == nil {
		r.logger.Error(wfCtx.Context(), "Plan activity returned nil result")
		r.recordRunStatus(wfCtx.Context(), input, run.StatusFailed, map[string]any{
			"error": "CRITICAL: Plan activity returned nil PlanResult",
		})
		return nil, fmt.Errorf("CRITICAL: Plan activity returned nil PlanResult")
	}
	r.logger.Info(wfCtx.Context(), "Plan activity completed", "tool_calls", len(result.ToolCalls), "final_response", result.FinalResponse != nil)
	// CRITICAL: Validate PlanResult structure - if planner returned ToolCalls, they should be present
	if len(result.ToolCalls) == 0 && result.FinalResponse == nil && result.Await == nil {
		r.recordRunStatus(wfCtx.Context(), input, run.StatusFailed, map[string]any{
			"error": "CRITICAL: PlanResult has no ToolCalls, FinalResponse, or Await",
		})
		return nil, fmt.Errorf("CRITICAL: PlanResult has no ToolCalls, FinalResponse, or Await - this should never happen")
	}
	// CRITICAL: If ToolCalls is empty but planner returned them, serialization may have failed
	if len(result.ToolCalls) == 0 && result.FinalResponse != nil {
		r.logger.Info(wfCtx.Context(), "PlanResult has FinalResponse but no ToolCalls - workflow will return early")
	}
	caps := initialCaps(reg.Policy)
	// Apply per-run cap overrides (run-level)
	if input.Policy.MaxToolCalls > 0 {
		caps.MaxToolCalls = input.Policy.MaxToolCalls
		caps.RemainingToolCalls = input.Policy.MaxToolCalls
	}
	if input.Policy.MaxConsecutiveFailedToolCalls > 0 {
		caps.MaxConsecutiveFailedToolCalls = input.Policy.MaxConsecutiveFailedToolCalls
		caps.RemainingConsecutiveFailedToolCalls = input.Policy.MaxConsecutiveFailedToolCalls
	}
	// Compute deadline with per-run override taking precedence over registration policy
	var deadline time.Time
	if input.Policy.TimeBudget > 0 {
		deadline = wfCtx.Now().Add(input.Policy.TimeBudget)
	} else if reg.Policy.TimeBudget > 0 {
		deadline = wfCtx.Now().Add(reg.Policy.TimeBudget)
	}
	nextAttempt := planInput.RunContext.Attempt + 1
	r.logger.Info(wfCtx.Context(), "Starting runLoop", "tool_calls", len(result.ToolCalls))
	out, err := r.runLoop(wfCtx, reg, input, planInput, result, caps, deadline, nextAttempt, seq, nil, ctrl)
	if err != nil {
		r.recordRunStatus(wfCtx.Context(), input, run.StatusFailed, map[string]any{
			"error": err.Error(),
		})
		return nil, err
	}
	r.recordRunStatus(wfCtx.Context(), input, run.StatusCompleted, nil)
	return out, nil
}

// runLoop executes the plan/tool/resume cycle until the planner returns a final response
// or a cap/deadline is exceeded. The seq parameter enables turn-based event sequencing.
func (r *Runtime) runLoop(
	wfCtx engine.WorkflowContext,
	reg AgentRegistration,
	input *RunInput,
	base planner.PlanInput,
	initial *planner.PlanResult,
	caps policy.CapsState,
	deadline time.Time,
	nextAttempt int,
	seq *turnSequencer,
	parentTracker *childTracker,
	ctrl *interrupt.Controller,
) (*RunOutput, error) {
	ctx := wfCtx.Context()
	if r.logger == nil {
		r.logger = telemetry.NoopLogger{}
	}
	// Aggregate usage during the run via a temporary subscriber.
	var aggUsage model.TokenUsage
	usageSub := r.registerUsageAggregator(ctx, input.RunID, input.AgentID, &aggUsage)
	if usageSub != nil {
		defer func() {
			_ = usageSub.Close()
		}()
	}
	if initial == nil {
		return nil, fmt.Errorf("CRITICAL: runLoop initial PlanResult is nil")
	}
	if len(initial.ToolCalls) == 0 && initial.FinalResponse == nil && initial.Await == nil {
		return nil, fmt.Errorf("CRITICAL: runLoop initial PlanResult has no ToolCalls, FinalResponse, or Await")
	}
	r.logger.Info(ctx, "runLoop starting iteration", "tool_calls", len(initial.ToolCalls), "final_response", initial.FinalResponse != nil, "await", initial.Await != nil)
	result := initial
	var lastToolResults []*planner.ToolResult
	for {
		r.logger.Info(ctx, "runLoop iteration", "tool_calls", len(result.ToolCalls), "final_response", result.FinalResponse != nil, "await", result.Await != nil)
		if err := r.handleInterrupts(wfCtx, input, &base, seq, ctrl, &nextAttempt); err != nil {
			return nil, err
		}
		if !deadline.IsZero() && wfCtx.Now().After(deadline) {
			// Time budget exceeded: request a final tool-free response from the planner.
			return r.finalizeWithPlanner(wfCtx, reg, input, base, lastToolResults, nextAttempt, seq, planner.TerminationReasonTimeBudget)
		}
		// Handle Await: publish await event, pause and wait for external input.
		if result.Await != nil {
			r.logger.Info(ctx, "PlanResult has Await, handling await")
			if ctrl == nil {
				return nil, errors.New("await not supported in inline runs")
			}
			// Publish paused event with a reason.
			reason := "await_external"
			if result.Await.Clarification != nil {
				reason = "await_clarification"
			}
			// If planner provided structured await info, emit a typed await event first.
			if result.Await.Clarification != nil {
				c := result.Await.Clarification
				r.publishHook(ctx, hooks.NewAwaitClarificationEvent(
					base.RunContext.RunID, base.Agent.ID(), c.ID, c.Question, c.MissingFields, c.RestrictToTool, c.ExampleInput,
				), seq)
			} else if result.Await.ExternalTools != nil {
				e := result.Await.ExternalTools
				items := make([]hooks.AwaitToolItem, 0, len(e.Items))
				for _, it := range e.Items {
					items = append(items, hooks.AwaitToolItem{
						ToolName:   it.Name,
						ToolCallID: it.ToolCallID,
						Payload:    it.Payload,
					})
				}
				r.publishHook(ctx, hooks.NewAwaitExternalToolsEvent(
					base.RunContext.RunID, base.Agent.ID(), e.ID, items,
				), seq)
			}
			r.publishHook(ctx, hooks.NewRunPausedEvent(base.RunContext.RunID, base.Agent.ID(), reason, "runtime", nil, nil), seq)
			// Block until the appropriate provide signal arrives.
			if result.Await.Clarification != nil {
				ans, err := ctrl.WaitProvideClarification(ctx)
				if err != nil {
					return nil, err
				}
				// Validate await ID when provided
				if c := result.Await.Clarification; c != nil && c.ID != "" && ans.ID != "" && ans.ID != c.ID {
					return nil, errors.New("unexpected await ID for clarification")
				}
				// Append the answer as a user message and continue planning.
				if strings.TrimSpace(ans.Answer) != "" {
					base.Messages = append(base.Messages, planner.AgentMessage{
						Role:    "user",
						Content: ans.Answer,
					})
				}
				// Set running and emit resumed
				r.recordRunStatus(ctx, input, run.StatusRunning, map[string]any{
					"resumed_by": "clarification",
				})
				r.publishHook(ctx, hooks.NewRunResumedEvent(base.RunContext.RunID, base.Agent.ID(), "clarification_provided", ans.RunID, ans.Labels, 1), seq)
				// Immediately PlanResume
				resumeCtx := base.RunContext
				resumeCtx.Attempt = nextAttempt
				nextAttempt++
				resumeReq := PlanActivityInput{
					AgentID:     base.Agent.ID(),
					RunID:       base.RunContext.RunID,
					Messages:    base.Messages,
					RunContext:  resumeCtx,
					ToolResults: nil,
				}
				var err2 error
				result, err2 = r.runPlanActivity(wfCtx, reg.ResumeActivityName, reg.ResumeActivityOptions, resumeReq)
				if err2 != nil {
					return nil, err2
				}
				continue
			} else if result.Await.ExternalTools != nil {
				rs, err := ctrl.WaitProvideToolResults(ctx)
				if err != nil {
					return nil, err
				}
				// Validate await ID when provided
				if e := result.Await.ExternalTools; e != nil && e.ID != "" && rs.ID != "" && rs.ID != e.ID {
					return nil, errors.New("unexpected await ID for external tools")
				}
				// Feed results into next PlanResume turn directly.
				lastToolResults = rs.Results
				// Advance to PlanResume immediately without executing internal tools.
				resumeCtx := base.RunContext
				resumeCtx.Attempt = nextAttempt
				nextAttempt++
				resumeReq := PlanActivityInput{
					AgentID:     base.Agent.ID(),
					RunID:       base.RunContext.RunID,
					Messages:    base.Messages,
					RunContext:  resumeCtx,
					ToolResults: lastToolResults,
				}
				// Set running and emit resumed
				r.recordRunStatus(ctx, input, run.StatusRunning, map[string]any{
					"resumed_by": "tool_results",
					"results":    len(lastToolResults),
				})
				r.publishHook(ctx, hooks.NewRunResumedEvent(base.RunContext.RunID, base.Agent.ID(), "tool_results_provided", input.RunID, nil, 0), seq)
				var err2 error
				result, err2 = r.runPlanActivity(wfCtx, reg.ResumeActivityName, reg.ResumeActivityOptions, resumeReq)
				if err2 != nil {
					return nil, err2
				}
				continue
			}
		}

		r.logger.Info(ctx, "Checking result.ToolCalls", "len", len(result.ToolCalls))

		if len(result.ToolCalls) == 0 {
			r.logger.Info(ctx, "No tool calls, checking FinalResponse")
			if result.FinalResponse == nil {
				r.logger.Error(ctx, "ERROR - Neither tool calls nor final response!")
				// CRITICAL: This error will be visible in workflow failure logs
				return nil, fmt.Errorf("CRITICAL: planner returned neither tool calls nor final response - ToolCalls=%d, FinalResponse=%v, Await=%v", len(result.ToolCalls), result.FinalResponse != nil, result.Await != nil)
			}
			r.publishHook(
				ctx,
				hooks.NewAssistantMessageEvent(
					base.RunContext.RunID,
					base.Agent.ID(),
					result.FinalResponse.Message.Content,
					nil,
				),
				seq,
			)
			for _, note := range result.Notes {
				r.publishHook(
					ctx,
					hooks.NewPlannerNoteEvent(
						base.RunContext.RunID,
						base.Agent.ID(),
						note.Text,
						note.Labels,
					),
					seq,
				)
			}
			return &RunOutput{
				AgentID:    base.Agent.ID(),
				RunID:      base.RunContext.RunID,
				Final:      result.FinalResponse.Message,
				ToolEvents: lastToolResults,
				Notes:      result.Notes,
				Usage:      aggUsage,
			}, nil
		}

		if caps.RemainingToolCalls == 0 && caps.MaxToolCalls > 0 {
			// Tool cap exhausted: request a final tool-free response from the planner.
			return r.finalizeWithPlanner(wfCtx, reg, input, base, lastToolResults, nextAttempt, seq, planner.TerminationReasonToolCap)
		}
		if !deadline.IsZero() && wfCtx.Now().After(deadline) {
			// Time budget exceeded after evaluating caps/policy
			return r.finalizeWithPlanner(
				wfCtx,
				reg,
				input,
				base,
				lastToolResults,
				nextAttempt,
				seq,
				planner.TerminationReasonTimeBudget,
			)
		}
		// Start with candidate tool calls from the planner.
		candidates := result.ToolCalls

		r.logger.Info(ctx, "Workflow received tool calls from planner", "count", len(candidates))
		// Apply per-run policy overrides (restrict tool and tag filters) before policy decision.
		if ov := input.Policy; (ov.RestrictToTool != "" || len(ov.AllowedTags) > 0 || len(ov.DeniedTags) > 0) && len(candidates) > 0 {
			r.logger.Info(ctx, "Applying per-run policy overrides", "restrict_to_tool", ov.RestrictToTool, "allowed_tags", ov.AllowedTags, "denied_tags", ov.DeniedTags)
			metas := r.toolMetadata(candidates)
			filtered := make([]planner.ToolRequest, 0, len(candidates))
			for i, call := range candidates {
				// RestrictToTool
				if ov.RestrictToTool != "" && call.Name != ov.RestrictToTool {
					r.logger.Info(ctx, "Tool filtered by RestrictToTool", "tool", call.Name)
					continue
				}
				// Tag allow/deny checks
				ok := true
				if len(ov.AllowedTags) > 0 || len(ov.DeniedTags) > 0 {
					tags := metas[i].Tags
					if len(ov.AllowedTags) > 0 {
						if !hasIntersection(tags, ov.AllowedTags) {
							r.logger.Info(ctx, "Tool filtered by AllowedTags", "tool", call.Name, "tags", tags, "required", ov.AllowedTags)
							ok = false
						}
					}
					if ok && len(ov.DeniedTags) > 0 {
						if hasIntersection(tags, ov.DeniedTags) {
							r.logger.Info(ctx, "Tool filtered by DeniedTags", "tool", call.Name, "tags", tags, "denied", ov.DeniedTags)
							ok = false
						}
					}
				}
				if ok {
					filtered = append(filtered, call)
				}
			}
			candidates = filtered
			r.logger.Info(ctx, "After per-run policy filtering", "candidates", len(candidates))
		}
		allowed := candidates
		if r.Policy != nil {
			r.logger.Info(ctx, "Applying runtime policy decision")
			decision, err := r.Policy.Decide(ctx, policy.Input{
				RunContext:    base.RunContext,
				Tools:         r.toolMetadata(candidates),
				RetryHint:     toPolicyRetryHint(result.RetryHint),
				RemainingCaps: caps,
				Requested:     toolHandles(candidates),
				Labels:        base.RunContext.Labels,
			})
			if err != nil {
				return nil, err
			}
			if len(decision.Labels) > 0 {
				base.RunContext.Labels = mergeLabels(base.RunContext.Labels, decision.Labels)
				input.Labels = mergeLabels(input.Labels, decision.Labels)
			}
			if decision.DisableTools {
				return nil, errors.New("tool execution disabled by policy")
			}
			if len(decision.AllowedTools) > 0 {
				allowed = filterToolCalls(allowed, decision.AllowedTools)
			}
			caps = mergeCaps(caps, decision.Caps)
			r.recordPolicyDecision(ctx, input, decision)
			r.publishHook(ctx, hooks.NewPolicyDecisionEvent(
				base.RunContext.RunID,
				base.Agent.ID(),
				decision.AllowedTools,
				caps,
				cloneLabels(decision.Labels),
				cloneMetadata(decision.Metadata),
			), seq)
		}
		if len(allowed) == 0 {
			r.logger.Error(ctx, "ERROR - No tools allowed for execution after filtering", "candidates", len(result.ToolCalls))
			return nil, errors.New("no tools allowed for execution")
		}
		r.logger.Info(ctx, "Executing allowed tool calls", "count", len(allowed))
		if parentTracker != nil {
			ids := collectToolCallIDs(allowed)
			if len(ids) > 0 && parentTracker.registerDiscovered(ids) {
				r.publishHook(ctx,
					hooks.NewToolCallUpdatedEvent(
						base.RunContext.RunID,
						base.Agent.ID(),
						parentTracker.parentToolCallID,
						parentTracker.currentTotal(),
					),
					seq,
				)
				parentTracker.markUpdated()
			}
		}
		// Per-turn max cap from overrides (applies before remaining run caps)
		if input.Policy.PerTurnMaxToolCalls > 0 && len(allowed) > input.Policy.PerTurnMaxToolCalls {
			allowed = allowed[:input.Policy.PerTurnMaxToolCalls]
		}
		if caps.MaxToolCalls > 0 && caps.RemainingToolCalls < len(allowed) {
			allowed = allowed[:caps.RemainingToolCalls]
		}
		for i := range allowed {
			if allowed[i].RunID == "" {
				allowed[i].RunID = base.RunContext.RunID
			}
			if allowed[i].SessionID == "" {
				allowed[i].SessionID = base.RunContext.SessionID
			}
			if allowed[i].TurnID == "" {
				allowed[i].TurnID = base.RunContext.TurnID
			}
			// Assign deterministic tool-call IDs and inherit parent when tracking children.
			if allowed[i].ToolCallID == "" {
				allowed[i].ToolCallID = generateDeterministicToolCallID(
					base.RunContext.RunID, base.RunContext.TurnID, allowed[i].Name, i,
				)
			}
			if parentTracker != nil && allowed[i].ParentToolCallID == "" {
				allowed[i].ParentToolCallID = parentTracker.parentToolCallID
			}
		}
		vals, err := r.executeToolCalls(
			wfCtx, reg.ExecuteToolActivity, reg.ExecuteToolActivityOptions, base.RunContext.RunID, base.Agent.ID(),
			allowed, result.ExpectedChildren, seq, parentTracker,
		)
		if err != nil {
			return nil, err
		}
		// Directly use pointer results for the planner input
		lastToolResults = vals
		// Decrement cap by the number of tool calls executed, not the number of results returned.
		// This ensures the cap is properly decremented even if some results are missing.
		caps.RemainingToolCalls = decrementCap(caps.RemainingToolCalls, len(allowed))
		if failures(vals) > 0 {
			caps.RemainingConsecutiveFailedToolCalls = decrementCap(
				caps.RemainingConsecutiveFailedToolCalls, failures(vals),
			)
			if caps.MaxConsecutiveFailedToolCalls > 0 && caps.RemainingConsecutiveFailedToolCalls <= 0 {
				// Consecutive failure cap exhausted: request finalization.
				return r.finalizeWithPlanner(wfCtx, reg, input, base, lastToolResults, nextAttempt, seq, planner.TerminationReasonFailureCap)
			}
		} else if caps.MaxConsecutiveFailedToolCalls > 0 {
			caps.RemainingConsecutiveFailedToolCalls = caps.MaxConsecutiveFailedToolCalls
		}

		// Apply missing-fields policy if configured. The helper may finalize or pause/resume.
		if out, err := r.handleMissingFieldsPolicy(wfCtx, reg, input, &base, vals, &nextAttempt, seq, ctrl); err != nil {
			return nil, err
		} else if out != nil {
			return out, nil
		}

		resumeCtx := base.RunContext
		resumeCtx.Attempt = nextAttempt
		nextAttempt++
		resumeReq := PlanActivityInput{
			AgentID:     base.Agent.ID(),
			RunID:       base.RunContext.RunID,
			Messages:    base.Messages,
			RunContext:  resumeCtx,
			ToolResults: lastToolResults,
		}
		result, err = r.runPlanActivity(wfCtx, reg.ResumeActivityName, reg.ResumeActivityOptions, resumeReq)
		if err != nil {
			return nil, err
		}
	}
}

// handleMissingFieldsPolicy inspects tool results for a RetryHint indicating missing
// required fields and applies the agent RunPolicy.OnMissingFields behavior:
//
//   - MissingFieldsFinalize: immediately finalize by requesting a tool-free final answer
//     from the planner. Returns a non-nil RunOutput to short-circuit the loop.
//   - MissingFieldsAwaitClarification: when durable (interrupt controller present), emit
//     an await_clarification event and pause the run. On resume, appends the user answer
//     as a message to the base PlanInput so the next turn can proceed. Returns handled=true.
//   - MissingFieldsResume (or unspecified): do nothing; the planner will see RetryHints
//     and may choose how to proceed. Returns handled=false.
//
// The function returns:
//   - out: non-nil only when finalization occurred
//   - handled: true when a pause/resume cycle was performed
//   - err: any error encountered while pausing/resuming
func (r *Runtime) handleMissingFieldsPolicy(
	wfCtx engine.WorkflowContext,
	reg AgentRegistration,
	input *RunInput,
	base *planner.PlanInput,
	results []*planner.ToolResult,
	nextAttempt *int,
	seq *turnSequencer,
	ctrl *interrupt.Controller,
) (*RunOutput, error) {
	if ctrl == nil || reg.Policy.OnMissingFields == "" {
		return nil, nil
	}
	ctx := wfCtx.Context()
	// Find first result with missing-fields hint and capture tool context.
	var (
		mf          *planner.RetryHint
		triggerTool tools.Ident
		triggerCall string
	)
	for _, tr := range results {
		if tr == nil || tr.RetryHint == nil {
			continue
		}
		if tr.RetryHint.Reason == planner.RetryReasonMissingFields || len(tr.RetryHint.MissingFields) > 0 {
			mf = tr.RetryHint
			triggerTool = tr.Name
			triggerCall = tr.ToolCallID
			break
		}
	}
	if mf == nil {
		return nil, nil
	}
	switch reg.Policy.OnMissingFields {
	case MissingFieldsFinalize:
		out, err := r.finalizeWithPlanner(wfCtx, reg, input, *base, results, *nextAttempt, seq, planner.TerminationReasonFailureCap)
		return out, err
	case MissingFieldsAwaitClarification:
		// Generate deterministic await ID for correlation safety.
		awaitID := generateDeterministicAwaitID(base.RunContext.RunID, base.RunContext.TurnID, triggerTool, triggerCall)
		var restrict tools.Ident
		if mf.RestrictToTool {
			restrict = mf.Tool
		}
		r.publishHook(ctx, hooks.NewAwaitClarificationEvent(
			base.RunContext.RunID,
			base.Agent.ID(),
			awaitID,
			mf.ClarifyingQuestion,
			mf.MissingFields,
			restrict,
			mf.ExampleInput,
		), seq)
		r.publishHook(ctx, hooks.NewRunPausedEvent(base.RunContext.RunID, base.Agent.ID(), "await_clarification", "runtime", nil, nil), seq)
		ans, err := ctrl.WaitProvideClarification(ctx)
		if err != nil {
			return nil, err
		}
		// Validate correlation when ID is present on the answer.
		if strings.TrimSpace(ans.ID) != "" && ans.ID != awaitID {
			return nil, fmt.Errorf("unexpected await ID for clarification")
		}
		if strings.TrimSpace(ans.Answer) != "" {
			base.Messages = append(base.Messages, planner.AgentMessage{
				Role:    "user",
				Content: ans.Answer,
			})
		}
		r.recordRunStatus(ctx, input, run.StatusRunning, map[string]any{
			"resumed_by": "clarification",
		})
		r.publishHook(ctx, hooks.NewRunResumedEvent(base.RunContext.RunID, base.Agent.ID(), "clarification_provided", input.RunID, ans.Labels, 1), seq)
		return nil, nil
	case MissingFieldsResume:
		return nil, nil
	default:
		return nil, nil
	}
}

// finalizeWithPlanner asks the planner for a tool-free final response and returns it as RunOutput.
func (r *Runtime) finalizeWithPlanner(
	wfCtx engine.WorkflowContext,
	reg AgentRegistration,
	input *RunInput,
	base planner.PlanInput,
	lastToolResults []*planner.ToolResult,
	nextAttempt int,
	seq *turnSequencer,
	reason planner.TerminationReason,
) (*RunOutput, error) {
	ctx := wfCtx.Context()
	// Prepare a brief message to steer planners that incorporate system messages.
	var hint string
	switch reason {
	case planner.TerminationReasonTimeBudget:
		hint = "Time budget reached. Provide the best possible final answer now. Do not call any tools."
	case planner.TerminationReasonToolCap:
		hint = "Tool budget exhausted. Provide the best possible final answer now. Do not call any tools."
	case planner.TerminationReasonFailureCap:
		hint = "Too many tool failures. Provide the best possible final answer now. Do not call any tools."
	default:
		hint = "Provide the best possible final answer now. Do not call any tools."
	}
	messages := base.Messages
	if strings.TrimSpace(hint) != "" {
		messages = append(messages, planner.AgentMessage{Role: "system", Content: hint})
	}
	resumeCtx := base.RunContext
	resumeCtx.Attempt = nextAttempt
	// Signal zero remaining duration for any prompt engineering that uses MaxDuration
	resumeCtx.MaxDuration = "0s"
	req := PlanActivityInput{
		AgentID:     base.Agent.ID(),
		RunID:       base.RunContext.RunID,
		Messages:    messages,
		RunContext:  resumeCtx,
		ToolResults: lastToolResults,
		Finalize:    &planner.Termination{Reason: reason, Message: hint},
	}
	// Emit a pause/resume pair to indicate a finalization turn began.
	r.publishHook(ctx, hooks.NewRunPausedEvent(base.RunContext.RunID, base.Agent.ID(), "finalize", "runtime", map[string]string{"reason": string(reason)}, nil), seq)
	r.recordRunStatus(ctx, input, run.StatusRunning, map[string]any{"resumed_by": "finalize"})
	r.publishHook(ctx, hooks.NewRunResumedEvent(base.RunContext.RunID, base.Agent.ID(), "finalize", input.RunID, nil, 0), seq)

	// Human‑readable reason strings for error contexts when finalization fails.
	reasonText := func() string {
		switch reason {
		case planner.TerminationReasonTimeBudget:
			return "time budget exceeded"
		case planner.TerminationReasonToolCap:
			return "tool call cap exceeded"
		case planner.TerminationReasonFailureCap:
			return "consecutive failed tool call cap exceeded"
		default:
			return "finalization failed"
		}
	}()

	result, err := r.runPlanActivity(wfCtx, reg.ResumeActivityName, reg.ResumeActivityOptions, req)
	if err != nil {
		// Surface the termination reason prominently; include underlying error for observability.
		return nil, fmt.Errorf("%s: %w", reasonText, err)
	}
	if result == nil || result.FinalResponse == nil {
		return nil, fmt.Errorf("%s", reasonText)
	}
	r.publishHook(
		ctx,
		hooks.NewAssistantMessageEvent(
			base.RunContext.RunID,
			base.Agent.ID(),
			result.FinalResponse.Message.Content,
			nil,
		),
		seq,
	)
	for _, note := range result.Notes {
		r.publishHook(
			ctx,
			hooks.NewPlannerNoteEvent(
				base.RunContext.RunID,
				base.Agent.ID(),
				note.Text,
				note.Labels,
			),
			seq,
		)
	}
	return &RunOutput{
		AgentID:    base.Agent.ID(),
		RunID:      base.RunContext.RunID,
		Final:      result.FinalResponse.Message,
		ToolEvents: lastToolResults,
		Notes:      result.Notes,
		// Usage aggregation continues to be recorded by the surrounding loop
	}, nil
}

func (r *Runtime) handleInterrupts(
	wfCtx engine.WorkflowContext,
	input *RunInput,
	base *planner.PlanInput,
	seq *turnSequencer,
	ctrl *interrupt.Controller,
	nextAttempt *int,
) error {
	if ctrl == nil {
		return nil
	}
	ctx := wfCtx.Context()
	for {
		req, ok := ctrl.PollPause()
		if !ok {
			break
		}
		r.recordRunStatus(ctx, input, run.StatusPaused, map[string]any{
			"reason": req.Reason,
		})
		r.publishHook(
			ctx,
			hooks.NewRunPausedEvent(
				input.RunID,
				input.AgentID,
				req.Reason,
				req.RequestedBy,
				req.Labels,
				req.Metadata,
			),
			seq,
		)

		resumeReq, err := ctrl.WaitResume(ctx)
		if err != nil {
			return err
		}
		if len(resumeReq.Messages) > 0 {
			base.Messages = append(base.Messages, resumeReq.Messages...)
		}
		base.RunContext.Attempt = *nextAttempt
		*nextAttempt++
		r.recordRunStatus(ctx, input, run.StatusRunning, map[string]any{
			"resumed_by": resumeReq.RequestedBy,
		})
		r.publishHook(
			ctx,
			hooks.NewRunResumedEvent(
				input.RunID,
				input.AgentID,
				resumeReq.Notes,
				resumeReq.RequestedBy,
				resumeReq.Labels,
				len(resumeReq.Messages),
			),
			seq,
		)
	}
	return nil
}

// executeToolCalls schedules tool activities in parallel and collects their results.
// Tools are launched asynchronously via ExecuteActivityAsync, then results are collected
// in order. This provides better performance for independent tool calls while maintaining
// deterministic result ordering. expectedChildren indicates how many child tools are expected
// to be discovered dynamically by the tools in this batch (0 if not tracked).
func collectToolCallIDs(calls []planner.ToolRequest) []string {
	ids := make([]string, 0, len(calls))
	for _, call := range calls {
		ids = append(ids, call.ToolCallID)
	}
	return ids
}

func (r *Runtime) executeToolCalls(
	wfCtx engine.WorkflowContext,
	activityName string, toolActOptions engine.ActivityOptions, runID, agentID string,
	calls []planner.ToolRequest,
	expectedChildren int,
	seq *turnSequencer,
	parentTracker *childTracker,
) ([]*planner.ToolResult, error) {
	if activityName == "" {
		return nil, errors.New("execute tool activity not registered")
	}
	ctx := wfCtx.Context()

	// Decide per-call execution path (inline vs activity) by toolset.
	// Inline toolsets run synchronously within the workflow loop; others schedule
	// activities and collect futures. Results are returned in call order.
	futures := make([]futureInfo, 0, len(calls))
	discoveredIDs := make([]string, 0, len(calls))
	inlineResults := make([]*planner.ToolResult, 0)
	for i, call := range calls {
		if call.ToolCallID == "" {
			call.ToolCallID = generateDeterministicToolCallID(runID, call.TurnID, call.Name, i)
			calls[i] = call
		}
		if parentTracker != nil && call.ParentToolCallID == "" {
			call.ParentToolCallID = parentTracker.parentToolCallID
			calls[i] = call
		}

		toolsetName := toolsetIdentifier(call.Name)
		ts, hasTS := r.toolsets[toolsetName]

		// Prepare scheduled event (queue filled only for activity execution)
		queue := ""
		if hasTS && ts.TaskQueue != "" {
			queue = ts.TaskQueue
		}
		r.publishHook(ctx,
			hooks.NewToolCallScheduledEvent(
				runID, agentID, call.Name, call.ToolCallID, call.Payload, queue,
				call.ParentToolCallID, expectedChildren,
			),
			seq,
		)

		if hasTS && ts.Inline {
			// Execute inline, preserving workflow context on ctx for agent-as-tool
			start := wfCtx.Now()
			ctxWithWF := engine.WithWorkflowContext(ctx, wfCtx)
			res, err := ts.Execute(ctxWithWF, call)
			if err != nil {
				return nil, fmt.Errorf("tool %q inline execution failed: %w", call.Name, err)
			}
			duration := wfCtx.Now().Sub(start)
			// Telemetry builder support can be applied post-aggregation where needed.
			// Emit result event with typed payload directly unless suppressed.
			if !ts.SuppressChildEvents {
				var evtErr *toolerrors.ToolError
				if res.Error != nil {
					evtErr = res.Error
				}
				r.publishHook(ctx,
					hooks.NewToolResultReceivedEvent(
						runID,
						agentID,
						call.Name,
						call.ToolCallID,
						call.ParentToolCallID,
						res.Result,
						duration,
						res.Telemetry,
						evtErr,
					),
					seq,
				)
			}
			// Accumulate result preserving order
			inlineResults = append(inlineResults, &planner.ToolResult{
				Name:       call.Name,
				Result:     res.Result,
				ToolCallID: call.ToolCallID,
				Telemetry:  res.Telemetry,
				Error:      res.Error,
				RetryHint:  res.RetryHint,
			})
			if parentTracker != nil {
				discoveredIDs = append(discoveredIDs, call.ToolCallID)
			}
			continue
		}

		// Activity path (default)
		rawPayload, err := r.marshalToolValue(ctx, call.Name, call.Payload, true)
		if err != nil {
			return nil, err
		}
		req := engine.ActivityRequest{
			Name: activityName,
			Input: ToolInput{
				AgentID:          agentID,
				RunID:            runID,
				ToolsetName:      toolsetName,
				ToolName:         call.Name,
				ToolCallID:       call.ToolCallID,
				Payload:          rawPayload,
				SessionID:        call.SessionID,
				TurnID:           call.TurnID,
				ParentToolCallID: call.ParentToolCallID,
			},
		}
		// Apply strong-contract execute_tool options first
		if toolActOptions.Queue != "" {
			req.Queue = toolActOptions.Queue
		}
		if toolActOptions.Timeout > 0 {
			req.Timeout = toolActOptions.Timeout
		}
		if !isZeroRetryPolicy(toolActOptions.RetryPolicy) {
			req.RetryPolicy = toolActOptions.RetryPolicy
		}
		// If no queue specified via options, allow explicit toolset TaskQueue to set it
		if req.Queue == "" && hasTS && ts.TaskQueue != "" {
			req.Queue = ts.TaskQueue
		}
		future, err := wfCtx.ExecuteActivityAsync(ctx, req)
		if err != nil {
			return nil, fmt.Errorf("failed to schedule tool %q: %w", call.Name, err)
		}
		futures = append(futures, futureInfo{
			future:    future,
			call:      call,
			startTime: wfCtx.Now(),
		})
		if parentTracker != nil {
			discoveredIDs = append(discoveredIDs, call.ToolCallID)
		}
	}

	if parentTracker != nil && parentTracker.registerDiscovered(discoveredIDs) && parentTracker.needsUpdate() {
		r.publishHook(
			ctx,
			hooks.NewToolCallUpdatedEvent(runID, agentID, parentTracker.parentToolCallID, parentTracker.currentTotal()),
			seq,
		)
		parentTracker.markUpdated()
	}

	// Collect all results in order
	results := make([]*planner.ToolResult, 0, len(calls))
	// First, append inline results in the same order they were executed relative to calls.
	// We'll merge activity results preserving call order below.
	// To keep ordering simple, we build a map from ToolCallID to inline result.
	inlineByID := make(map[string]*planner.ToolResult, len(inlineResults))
	for _, ir := range inlineResults {
		if ir != nil {
			inlineByID[ir.ToolCallID] = ir
		}
	}
	// Collect activity results
	activityByID := make(map[string]*planner.ToolResult, len(futures))
	for _, info := range futures {
		var out ToolOutput
		if err := info.future.Get(ctx, &out); err != nil {
			return nil, fmt.Errorf("tool %q failed: %w", info.call.Name, err)
		}

		duration := wfCtx.Now().Sub(info.startTime)
		// Decode tool result. If decoding fails (mismatch with generated result
		// type), do not abort the workflow. Forward the raw payload to preserve
		// observability and allow clients to render best‑effort data.
		var decoded any
		if len(out.Payload) > 0 {
			if v, decErr := r.unmarshalToolValue(ctx, info.call.Name, out.Payload, false); decErr == nil {
				decoded = v
			} else {
				decoded = out.Payload
			}
		}

		toolRes := &planner.ToolResult{
			Name:       info.call.Name,
			Result:     decoded,
			ToolCallID: info.call.ToolCallID,
			Telemetry:  out.Telemetry,
		}
		var toolErr *planner.ToolError
		if out.Error != "" {
			toolErr = planner.NewToolError(out.Error)
			toolRes.Error = toolErr
		}
		if out.RetryHint != nil {
			toolRes.RetryHint = out.RetryHint
		}

		r.publishHook(
			ctx,
			hooks.NewToolResultReceivedEvent(
				runID,
				agentID,
				info.call.Name,
				info.call.ToolCallID,
				info.call.ParentToolCallID,
				decoded,
				duration,
				out.Telemetry,
				toolErr,
			),
			seq,
		)

		activityByID[info.call.ToolCallID] = toolRes
	}

	// Merge results following original call order (inline or activity)
	for _, call := range calls {
		if ir, ok := inlineByID[call.ToolCallID]; ok {
			results = append(results, ir)
			continue
		}
		if ar, ok := activityByID[call.ToolCallID]; ok {
			results = append(results, ar)
			continue
		}
		// Should not happen; defensive: keep order consistent even if missing.
		r.logWarn(ctx, "missing tool result for call", fmt.Errorf("no result"), "tool", call.Name, "tool_call_id", call.ToolCallID)
	}

	return results, nil
}

// runPlanActivity schedules a plan/resume activity with the configured options.
func (r *Runtime) runPlanActivity(
	wfCtx engine.WorkflowContext, activityName string, options engine.ActivityOptions, input PlanActivityInput,
) (*planner.PlanResult, error) {
	if activityName == "" {
		return nil, errors.New("plan activity not registered")
	}
	var out PlanActivityOutput
	req := engine.ActivityRequest{
		Name:  activityName,
		Input: input,
	}
	if options.Queue != "" {
		req.Queue = options.Queue
	}
	if options.Timeout > 0 {
		req.Timeout = options.Timeout
	}
	if !isZeroRetryPolicy(options.RetryPolicy) {
		req.RetryPolicy = options.RetryPolicy
	}
	if err := wfCtx.ExecuteActivity(wfCtx.Context(), req, &out); err != nil {
		return nil, err
	}
	if out.Result == nil {
		return nil, fmt.Errorf("CRITICAL: runPlanActivity received nil PlanResult")
	}
	if len(out.Result.ToolCalls) == 0 && out.Result.FinalResponse == nil && out.Result.Await == nil {
		return nil, fmt.Errorf("CRITICAL: runPlanActivity received PlanResult with no ToolCalls, FinalResponse, or Await")
	}
	r.logger.Info(wfCtx.Context(), "runPlanActivity received PlanResult", "tool_calls", len(out.Result.ToolCalls), "final_response", out.Result.FinalResponse != nil, "await", out.Result.Await != nil)
	return out.Result, nil
}

// recordRunStatus upserts run metadata to the store if configured.
func (r *Runtime) recordRunStatus(ctx context.Context, input *RunInput, status run.Status, meta map[string]any) {
	if r.RunStore == nil {
		return
	}
	rec := run.Record{
		AgentID:   input.AgentID,
		RunID:     input.RunID,
		SessionID: input.SessionID,
		TurnID:    input.TurnID,
		Status:    status,
		StartedAt: time.Now(),
		UpdatedAt: time.Now(),
		Labels:    cloneLabels(input.Labels),
		Metadata:  meta,
	}
	if err := r.RunStore.Upsert(ctx, rec); err != nil {
		r.logWarn(ctx, "run record upsert failed", err)
	}
}

func (r *Runtime) recordPolicyDecision(ctx context.Context, input *RunInput, decision policy.Decision) {
	if r.RunStore == nil {
		return
	}
	rec, err := r.RunStore.Load(ctx, input.RunID)
	if err != nil {
		r.logWarn(ctx, "run record load failed", err, "run_id", input.RunID)
		return
	}
	now := time.Now()
	if rec.RunID == "" {
		rec.AgentID = input.AgentID
		rec.RunID = input.RunID
		rec.SessionID = input.SessionID
		rec.TurnID = input.TurnID
		rec.StartedAt = now
	}
	if rec.StartedAt.IsZero() {
		rec.StartedAt = now
	}
	rec.AgentID = input.AgentID
	rec.SessionID = input.SessionID
	rec.TurnID = input.TurnID
	rec.Status = run.StatusRunning
	rec.UpdatedAt = now
	rec.Labels = mergeLabels(rec.Labels, input.Labels)

	entry := map[string]any{
		"caps":      decision.Caps,
		"timestamp": now.UTC(),
	}
	if len(decision.AllowedTools) > 0 {
		entry["allowed_tools"] = decision.AllowedTools
	}
	if len(decision.Labels) > 0 {
		entry["labels"] = cloneLabels(decision.Labels)
	}
	if len(decision.Metadata) > 0 {
		entry["metadata"] = cloneMetadata(decision.Metadata)
	}
	if decision.DisableTools {
		entry["disable_tools"] = true
	}

	meta := cloneMetadata(rec.Metadata)
	meta = appendPolicyDecisionMetadata(meta, entry)
	rec.Metadata = meta

	if err := r.RunStore.Upsert(ctx, rec); err != nil {
		r.logWarn(ctx, "policy decision upsert failed", err)
	}
}

// registerUsageAggregator subscribes to Usage events for a specific run/agent and
// aggregates token usage into the provided accumulator. Returns a subscription
// handle that should be closed by the caller. If the bus is nil or registration
// fails, returns nil.
func (r *Runtime) registerUsageAggregator(
	ctx context.Context,
	runID string,
	agentID string,
	agg *model.TokenUsage,
) hooks.Subscription {
	if r.Bus == nil {
		return nil
	}
	sub, err := r.Bus.Register(hooks.SubscriberFunc(func(c context.Context, evt hooks.Event) error {
		if evt.RunID() != runID || evt.AgentID() != agentID {
			return nil
		}
		if u, ok := evt.(*hooks.UsageEvent); ok {
			*agg = model.TokenUsage{
				InputTokens:  agg.InputTokens + u.InputTokens,
				OutputTokens: agg.OutputTokens + u.OutputTokens,
				TotalTokens:  agg.TotalTokens + u.TotalTokens,
			}
		}
		return nil
	}))
	if err != nil {
		r.logWarn(ctx, "usage subscriber register failed", err)
		return nil
	}
	return sub
}

// memoryReader loads the run snapshot from the memory store and wraps it in a Reader.
func (r *Runtime) memoryReader(ctx context.Context, agentID, runID string) memory.Reader {
	if r.Memory == nil {
		return emptyMemoryReader{}
	}
	snapshot, err := r.Memory.LoadRun(ctx, agentID, runID)
	if err != nil {
		return emptyMemoryReader{}
	}
	return newMemoryReader(snapshot.Events)
}

// generateRunID creates a unique run identifier by combining the agent ID and a UUID.
func generateRunID(agentID string) string {
	prefix := strings.ReplaceAll(agentID, ".", "-")
	return fmt.Sprintf("%s-%s", prefix, uuid.NewString())
}

// nextSeq increments and returns the next sequence number for this turn.
func (t *turnSequencer) nextSeq() int {
	seq := t.sequence
	t.sequence++
	return seq
}
